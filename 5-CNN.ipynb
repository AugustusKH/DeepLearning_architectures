{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5At28uyZsEh"
      },
      "source": [
        "Department of Mathematics and Computer Science, Faculty of Science, Chulalongkorn University\n",
        "\n",
        "2301648 Architecture of Deep Learning\n",
        "\n",
        "On-line final exam, Saturday 27 April 2024, 8:30 - 11:30  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Must install numpy version 1.23.1 and mxnet and then restart the session after the installation."
      ],
      "metadata": {
        "id": "F0EKcz7U6U0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet numpy==1.23.1 mxnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_jHLk35NHKD",
        "outputId": "45956f06-239d-4349-b185-7becc2100e16"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.23.1 which is incompatible.\n",
            "pandas-stubs 2.0.3.230814 requires numpy>=1.25.0; python_version >= \"3.9\", but you have numpy 1.23.1 which is incompatible.\n",
            "tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 1.23.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\" size=\"10\">Must restart session:Runtime --> Restart session</font>"
      ],
      "metadata": {
        "id": "rgW9I8tp6cxo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mAEonxvZsEs"
      },
      "source": [
        "**Instructions**\n",
        "\n",
        "1. There are 4 questions in this exam.\n",
        "2. Type in your Student ID and full name in the box below.\n",
        "3. You must not distribute any part of this exam through any other persons. The exam is a government's property. Violators will be prosecuted under a criminal court.\n",
        "4. Once the time is expired, student must stop typing .\n",
        "5. Any student who does not obey the regulations listed above will receive punishment under the Faculty of Science Official Announcement regarding the exam regulations.\n",
        "\n",
        "** With implicit evidence or showing intention for cheating, student will receive an F in this course and will receive an academic suspension for 1 semester.**\n",
        "\n",
        "**With explicit evidence for cheating, student will receive an F in this course and will receive an academic suspension for 1 year.**\n",
        "\n",
        "I acknowledge all instructions above. This exam represents only my own work. I did not give or receive help on this exam.\n",
        "\n",
        "Type in your name and date below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uzWzmZDZsEt"
      },
      "source": [
        "### Type in your student ID and Name here\n",
        "**ID** 6581030520\n",
        "**Name** Pakorn Sagulkoo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCEU172qZsEu"
      },
      "source": [
        "%matplotlib inline\n",
        "import re\n",
        "import mxnet as mx\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from mxnet import nd, autograd, gluon\n",
        "from mxnet.gluon import nn, loss\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import fetch_openml"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the digits dataset from scikit learning library\n",
        "\n",
        "\n",
        "The USPS digits dataset or simply USPS dataset is a collection of handwritten digits collected from US postal service. It is commonly used as a benchmark dataset in machine learning and pattern recognition research, particularly for tasks related to digit recognition or classification.\n",
        "\n",
        "The dataset consists of grayscale images of handwritten digits from 0 to 9 indexing as 1 to 10. The images are of size 16x16 pixels of 1 channel having 9298 images. Pixel values range from 0 (black) to 255 (white)."
      ],
      "metadata": {
        "id": "CC5qSnAS9TsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and load the USPS dataset\n",
        "usps = fetch_openml('usps', version=2, data_home='.')\n",
        "\n",
        "# Transform features and target\n",
        "X, y = (usps.data+1)/2.0, usps.target.astype(np.int32)-1\n",
        "\n",
        "# Split data into training and testing sets (optional)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)\n",
        "\n",
        "# Transform to NDArray\n",
        "X_train_mx, X_test_mx = nd.array(X_train).reshape((-1,1,16,16)), nd.array(X_test).reshape((-1,1,16,16))\n",
        "y_train_mx, y_test_mx = nd.array(y_train), nd.array(y_test)"
      ],
      "metadata": {
        "id": "WWjTiG_V7htQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c40bfe7-d971-4f65-fccf-5ac4ca19dca4"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X), X.shape)\n",
        "print(len(y), y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRjLpjuy863o",
        "outputId": "6c3b7ab5-c84e-44e6-eb93-22f74cb025cd"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9298 (9298, 256)\n",
            "9298 (9298,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlJBtTs6ZsEw"
      },
      "source": [
        "## 1. Convolutional neural network architecture (10 points)\n",
        "\n",
        "Fill-in the following convolutional neural network architecture layer-by-layer. You will need to determine the dimension of the output and the number of parameters in each layer. You could use the format (H, W, C) for representing the height, width and channel dimensions. Without specific padding and stride, assume padding of 0 and stride of 1.\n",
        "\n",
        "**Notation**\n",
        "- CONV-$s$_$N$ represents the $N$ filter matrices of size $s \\times s$.  \n",
        "- RELU represents the relu activation function  \n",
        "- MAXPOOL-$s$   represents the max pooling of size $s \\times s$\n",
        "- FLATTEN represents the conversion from any array into an array of 1 dimension\n",
        "- FC-$N$ represents the fully connected layer of neural network having $N$ nodes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axtChi3oZsEx"
      },
      "source": [
        "## Solution 1. (Your complete solution must type in this box.)\n",
        "\n",
        "| **Layer** | **Dimension** &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | **Number of parameters** |\n",
        "|-----------|---------------|--------------------------|\n",
        "| INPUT | $16 \\times 16 \\times 1$ | 0 |\n",
        "| CONV-3_8 | $14 \\times 14 \\times 8$ | 72 |\n",
        "| RELU    | $14 \\times 14 \\times 8$ | 0 |\n",
        "| MAXPOOL-2 (Padding=0, Stride=2)  | $7 \\times 7 \\times 8$ | 0 |\n",
        "| CONV-3_4 (Padding=1, Stride=2)  | $4 \\times 4 \\times 4$ | 288 |\n",
        "| RELU    | $4 \\times 4 \\times 4$ | 0  |\n",
        "| MAXPOOL-2 (Padding=1, Stride=2)  | $3 \\times 3 \\times 4$ | 0 |\n",
        "| FLATTEN | $36 \\times 1$ | 0  |\n",
        "| FULLYCON-64 | $64 \\times 1$ |  2368  |\n",
        "| FULLYCON-10 | $10 \\times 1$ |  650  |\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oS1mw68ZsEy"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4eky8N-ZsE1"
      },
      "source": [
        "## 2. CNN from scratch (10 points)\n",
        "From the following CNN code with Relu activation function, you need to adapt this CNN code and determine the appropriate hyper-parameters for training this CNN to solve the USPS digits classification problem. Change ***XXX*** to your parameter settings.\n",
        "\n",
        "**Score**\n",
        "- 5 points will be giving to the success of implementing CNN.  \n",
        "- Additional points will be given as follow\n",
        "   - 5 points will be giving to training with test accuracy > 95%  \n",
        "   - 4 points will be giving to training with test accuracy in [90, 95]%    \n",
        "   - 3 points will be giving to training with test accuracy in [80, 89]%  \n",
        "   - 2 points will be giving to training with test accuracy in [70, 79]%  \n",
        "   - 1 points will be giving to training with test accuracy in [50, 69]%  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGj9LcSnZsE2"
      },
      "source": [
        "# Set up the cpu and seed\n",
        "ctx = mx.cpu()\n",
        "mx.random.seed(1)\n",
        "\n",
        "# Define data iterators (loaders)\n",
        "batch_size, num_input, num_outputs = 64, 9298, 10\n",
        "ntest = len(y_test_mx)\n",
        "train_iter = mx.gluon.data.DataLoader(mx.gluon.data.ArrayDataset(X_train_mx, y_train_mx), batch_size=batch_size, shuffle=True)\n",
        "test_iter = mx.gluon.data.DataLoader(mx.gluon.data.ArrayDataset(X_test_mx, y_test_mx), batch_size=ntest, shuffle=False)"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVNaZznuZsE4"
      },
      "source": [
        "weight_scale, num_fc = 0.01, 128\n",
        "num_filter_conv_layer1, num_filter_conv_layer2 = 20, 20\n",
        "\n",
        "W1 = nd.random_normal(shape=(num_filter_conv_layer1, 1, 3, 3),scale=weight_scale, ctx=ctx)\n",
        "b1 = nd.random_normal(shape=num_filter_conv_layer1, scale=weight_scale, ctx=ctx)\n",
        "\n",
        "W2 = nd.random_normal(shape=(num_filter_conv_layer2, num_filter_conv_layer1, 5, 5), scale=weight_scale, ctx=ctx)\n",
        "b2 = nd.random_normal(shape=num_filter_conv_layer2, scale=weight_scale, ctx=ctx)\n",
        "\n",
        "W3 = nd.random_normal(shape=(720, num_fc), scale=weight_scale, ctx=ctx)\n",
        "b3 = nd.random_normal(shape=num_fc, scale=weight_scale, ctx=ctx)\n",
        "\n",
        "W4 = nd.random_normal(shape=(num_fc, num_outputs), scale=weight_scale, ctx=ctx)\n",
        "b4 = nd.random_normal(shape=num_outputs, scale=weight_scale, ctx=ctx)\n",
        "\n",
        "params = [W1, b1, W2, b2, W3, b3, W4, b4]\n",
        "for param in params:\n",
        "    param.attach_grad()"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKw75Gs0ZsFE"
      },
      "source": [
        "def net(X, debug=False):\n",
        "    #  Define the computation of the first convolutional layer\n",
        "    h1_conv = nd.Convolution(data=X, weight=W1, bias=b1, kernel=(3, 3), num_filter=num_filter_conv_layer1)\n",
        "    h1_activation = relu(h1_conv)\n",
        "    h1 = nd.Pooling(data=h1_activation, pool_type=\"max\", kernel=(3, 3),stride=(1, 1))\n",
        "    if debug:\n",
        "        print(\"h1 shape: %s\" % (np.array(h1.shape)))\n",
        "    #  Define the computation of the second convolutional layer\n",
        "    h2_conv = nd.Convolution(data=h1, weight=W2, bias=b2, kernel=(5, 5), pad=(2, 2),\n",
        "                                  num_filter=num_filter_conv_layer2)\n",
        "    h2_activation = relu(h2_conv)\n",
        "    h2 = nd.Pooling(data=h2_activation, pool_type=\"max\", kernel=(2, 2), pad=(0, 0), stride=(2, 2))\n",
        "    if debug:\n",
        "        print(\"h2 shape: %s\" % (np.array(h2.shape)))\n",
        "    #  Flattening h2 so that we can feed it into a fully-connected layer\n",
        "    h2 = nd.flatten(h2)\n",
        "    if debug:\n",
        "        print(\"Flat h2 shape: %s\" % (np.array(h2.shape)))\n",
        "    #  Define the computation of the third (fully-connected) layer\n",
        "    h3_linear = nd.dot(h2, W3) + b3\n",
        "    h3 = relu(h3_linear)\n",
        "    if debug:\n",
        "        print(\"h3 shape: %s\" % (np.array(h3.shape)))\n",
        "    #  Define the computation of the output layer\n",
        "    yhat_linear = nd.dot(h3, W4) + b4\n",
        "    if debug:\n",
        "        print(\"yhat_linear shape: %s\" % (np.array(yhat_linear.shape)))\n",
        "    return yhat_linear"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## DO NOT CHANGE THIS BLOCK ##\n",
        "# Activation function\n",
        "def relu(X):\n",
        "    return nd.maximum(X,nd.zeros_like(X))\n",
        "# Softmax\n",
        "def softmax(y_linear):\n",
        "    exp = nd.exp(y_linear-nd.max(y_linear))\n",
        "    partition = nd.sum(exp, axis=0, exclude=True).reshape((-1,1))\n",
        "    return exp / partition\n",
        "# Softmax cross-entropy loss\n",
        "def softmax_cross_entropy(yhat_linear, y):\n",
        "    return - nd.nansum(y * nd.log_softmax(yhat_linear), axis=0, exclude=True)\n",
        "def SGD(params, lr):\n",
        "    for param in params:\n",
        "        param[:] = param - lr * param.grad\n",
        "\n",
        "def evaluate_accuracy(data_iterator, net):\n",
        "    numerator = 0.\n",
        "    denominator = 0.\n",
        "    for i, (data, label) in enumerate(data_iterator):\n",
        "        data = data.as_in_context(ctx)\n",
        "        label = label.as_in_context(ctx).astype(np.float32)\n",
        "        label_one_hot = nd.one_hot(label, 10)\n",
        "        output = net(data)\n",
        "        predictions = nd.argmax(output, axis=1).astype(np.float32)\n",
        "        numerator += nd.sum(predictions == label)\n",
        "        denominator += data.shape[0]\n",
        "    return (numerator / denominator).asscalar()"
      ],
      "metadata": {
        "id": "aOulSwoMws3f"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Before training check the accuracy of the random network on Test\n",
        "evaluate_accuracy(test_iter, net)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zFgnFIZJ23m",
        "outputId": "a6310241-c33f-4172-c646-a61286c4aa81"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.16702509"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs, learning_rate, smoothing_constant = 10, 0.5, 0.001\n",
        "for e in range(epochs):\n",
        "    for i, (data, label) in enumerate(train_iter):\n",
        "        label_one_hot = nd.one_hot(label.astype(np.float32), num_outputs)\n",
        "        with autograd.record():\n",
        "            output = net(data).astype(np.float32)\n",
        "            loss = softmax_cross_entropy(output, label_one_hot)\n",
        "        loss.backward()\n",
        "        SGD(params, learning_rate)\n",
        "\n",
        "        ##########################\n",
        "        #  Keep a moving average of the losses\n",
        "        ##########################\n",
        "        curr_loss = nd.mean(loss).asscalar()\n",
        "        moving_loss = (curr_loss if ((i == 0) and (e == 0))\n",
        "                       else (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss)\n",
        "\n",
        "    test_accuracy = evaluate_accuracy(test_iter, net)\n",
        "    train_accuracy = evaluate_accuracy(train_iter, net)\n",
        "    print(\"Epoch %d. Loss: %8.4F, Train_acc %9.4F, Test_acc %9.4F\" % (e+1, moving_loss.astype(np.float32), train_accuracy, test_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jbvi4I07OuON",
        "outputId": "c0ff719e-ee21-4e75-a3c4-ac75d80f9088"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1. Loss:   2.7432, Train_acc    0.0887, Test_acc    0.0885\n",
            "Epoch 2. Loss:   3.2029, Train_acc    0.0770, Test_acc    0.0771\n",
            "Epoch 3. Loss:   3.5987, Train_acc    0.0762, Test_acc    0.0760\n",
            "Epoch 4. Loss:   4.0111, Train_acc    0.1364, Test_acc    0.1366\n",
            "Epoch 5. Loss:   4.3960, Train_acc    0.0897, Test_acc    0.0896\n",
            "Epoch 6. Loss:   4.6893, Train_acc    0.0999, Test_acc    0.1000\n",
            "Epoch 7. Loss:   5.0271, Train_acc    0.1364, Test_acc    0.1366\n",
            "Epoch 8. Loss:   5.2634, Train_acc    0.0762, Test_acc    0.0760\n",
            "Epoch 9. Loss:   5.4835, Train_acc    0.0999, Test_acc    0.1000\n",
            "Epoch 10. Loss:   5.7051, Train_acc    0.0770, Test_acc    0.0771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kokeKiNKOuVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICAB_43QZsFP"
      },
      "source": [
        "## 3. CNN (10 points)\n",
        "\n",
        "Design the best CNN to learn USPS digits classification problem.\n",
        "\n",
        "**Score**\n",
        "- 5 points will be giving to CNN that works.\n",
        "- Additional points will be given as follow\n",
        "   - 5 points will be giving to training with test accuracy > 99%  \n",
        "   - 4 points will be giving to training with test accuracy in [95, 99)%    \n",
        "   - 3 points will be giving to training with test accuracy in [90, 95)%  \n",
        "   - 2 points will be giving to training with test accuracy in [80, 90)%  \n",
        "   - 1 points will be giving to training with test accuracy in [50, 80)%  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_fc = 512\n",
        "net = gluon.nn.Sequential()\n",
        "with net.name_scope():\n",
        "    net.add(gluon.nn.Conv2D(channels=20, kernel_size=3, padding=1, activation='relu'))\n",
        "    net.add(gluon.nn.MaxPool2D(pool_size=2, strides=2))\n",
        "    net.add(gluon.nn.Conv2D(channels=50, kernel_size=3, padding=1, activation='relu'))\n",
        "    net.add(gluon.nn.MaxPool2D(pool_size=2, strides=2))\n",
        "    # The Flatten layer collapses all axis, except the first one, into one axis.\n",
        "    net.add(gluon.nn.Flatten())\n",
        "    net.add(gluon.nn.Dense(num_fc, activation=\"relu\"))\n",
        "    net.add(gluon.nn.Dense(num_outputs))"
      ],
      "metadata": {
        "id": "j5t7JCaUbDc1"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set hyperparameters and weight initialization\n",
        "net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)\n",
        "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
        "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': .1})"
      ],
      "metadata": {
        "id": "VGUonmCObIGZ"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## DO NOT CHANGE THIS BLOCK ###\n",
        "def evaluate_accuracy(data_iterator, net):\n",
        "    acc = mx.metric.Accuracy()\n",
        "    for i, (data, label) in enumerate(data_iterator):\n",
        "        data = data.as_in_context(ctx)\n",
        "        label = label.as_in_context(ctx)\n",
        "        output = net(data)\n",
        "        predictions = nd.argmax(output, axis=1)\n",
        "        acc.update(preds=predictions, labels=label)\n",
        "    return acc.get()[1]"
      ],
      "metadata": {
        "id": "bM5Xziqjh9rG"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Before training check the accuracy of the random network on Test\n",
        "evaluate_accuracy(test_iter, net)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yClk49a4vSgw",
        "outputId": "cd9db098-3e9d-4eeb-d874-cafcfeb04d09"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.08136200716845877"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs, smoothing_constant = 10, .01\n",
        "for e in range(epochs):\n",
        "    for i, (data, label) in enumerate(train_iter):\n",
        "        data = data.as_in_context(ctx)\n",
        "        label = label.as_in_context(ctx)\n",
        "        with autograd.record():\n",
        "            output = net(data)\n",
        "            loss = softmax_cross_entropy(output, label)\n",
        "        loss.backward()\n",
        "        trainer.step(data.shape[0])\n",
        "        #  Keep a moving average of the losses\n",
        "        curr_loss = nd.mean(loss).asscalar()\n",
        "        moving_loss = (curr_loss if ((i == 0) and (e == 0))\n",
        "                       else (1 - smoothing_constant) * moving_loss + smoothing_constant * curr_loss)\n",
        "    test_accuracy = evaluate_accuracy(test_iter, net)\n",
        "    train_accuracy = evaluate_accuracy(train_iter, net)\n",
        "    print(\"Epoch %d. Loss: %9.4f, Train_acc %9.4f, Test_acc %9.4f\" % (e+1, moving_loss, train_accuracy, test_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mratyRLTcSVq",
        "outputId": "22c3bda3-e31f-4e90-b7d7-48c9b2761a1d"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1. Loss:    0.0179, Train_acc    0.9883, Test_acc    0.9699\n",
            "Epoch 2. Loss:    0.0201, Train_acc    0.9971, Test_acc    0.9799\n",
            "Epoch 3. Loss:    0.0188, Train_acc    0.9963, Test_acc    0.9763\n",
            "Epoch 4. Loss:    0.0195, Train_acc    0.9978, Test_acc    0.9760\n",
            "Epoch 5. Loss:    0.0187, Train_acc    0.9969, Test_acc    0.9771\n",
            "Epoch 6. Loss:    0.0158, Train_acc    0.9983, Test_acc    0.9781\n",
            "Epoch 7. Loss:    0.0118, Train_acc    0.9905, Test_acc    0.9659\n",
            "Epoch 8. Loss:    0.0124, Train_acc    0.9988, Test_acc    0.9781\n",
            "Epoch 9. Loss:    0.0108, Train_acc    0.9963, Test_acc    0.9728\n",
            "Epoch 10. Loss:    0.0081, Train_acc    0.9974, Test_acc    0.9731\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs, smoothing_constant = 20, .0001\n",
        "for e in range(epochs):\n",
        "    for i, (data, label) in enumerate(train_iter):\n",
        "        data = data.as_in_context(ctx)\n",
        "        label = label.as_in_context(ctx)\n",
        "        with autograd.record():\n",
        "            output = net(data)\n",
        "            loss = softmax_cross_entropy(output, label)\n",
        "        loss.backward()\n",
        "        trainer.step(data.shape[0])\n",
        "        #  Keep a moving average of the losses\n",
        "        curr_loss = nd.mean(loss).asscalar()\n",
        "        moving_loss = (curr_loss if ((i == 0) and (e == 0))\n",
        "                       else (1 - smoothing_constant) * moving_loss + smoothing_constant * curr_loss)\n",
        "    test_accuracy = evaluate_accuracy(test_iter, net)\n",
        "    train_accuracy = evaluate_accuracy(train_iter, net)\n",
        "    print(\"Epoch %d. Loss: %9.4f, Train_acc %9.4f, Test_acc %9.4f\" % (e+1, moving_loss, train_accuracy, test_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVOSQgVfxE9W",
        "outputId": "b809acd5-e418-4e8d-be0c-afd82d5fcfba"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1. Loss:    0.0011, Train_acc    0.9998, Test_acc    0.9785\n",
            "Epoch 2. Loss:    0.0011, Train_acc    0.9997, Test_acc    0.9771\n",
            "Epoch 3. Loss:    0.0012, Train_acc    0.9998, Test_acc    0.9789\n",
            "Epoch 4. Loss:    0.0012, Train_acc    0.9998, Test_acc    0.9792\n",
            "Epoch 5. Loss:    0.0012, Train_acc    0.9998, Test_acc    0.9785\n",
            "Epoch 6. Loss:    0.0012, Train_acc    0.9998, Test_acc    0.9785\n",
            "Epoch 7. Loss:    0.0039, Train_acc    0.9879, Test_acc    0.9692\n",
            "Epoch 8. Loss:    0.0042, Train_acc    0.9959, Test_acc    0.9749\n",
            "Epoch 9. Loss:    0.0044, Train_acc    0.9315, Test_acc    0.9136\n",
            "Epoch 10. Loss:    0.0046, Train_acc    0.9988, Test_acc    0.9760\n",
            "Epoch 11. Loss:    0.0046, Train_acc    0.9994, Test_acc    0.9746\n",
            "Epoch 12. Loss:    0.0046, Train_acc    0.9997, Test_acc    0.9767\n",
            "Epoch 13. Loss:    0.0047, Train_acc    0.9997, Test_acc    0.9763\n",
            "Epoch 14. Loss:    0.0047, Train_acc    0.9998, Test_acc    0.9746\n",
            "Epoch 15. Loss:    0.0047, Train_acc    0.9998, Test_acc    0.9756\n",
            "Epoch 16. Loss:    0.0046, Train_acc    0.9992, Test_acc    0.9767\n",
            "Epoch 17. Loss:    0.0046, Train_acc    0.9998, Test_acc    0.9763\n",
            "Epoch 18. Loss:    0.0046, Train_acc    0.9998, Test_acc    0.9742\n",
            "Epoch 19. Loss:    0.0046, Train_acc    0.9998, Test_acc    0.9756\n",
            "Epoch 20. Loss:    0.0046, Train_acc    1.0000, Test_acc    0.9763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RuyWOvP3Rmm"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwUh6hMfZsFX"
      },
      "source": [
        "## 4. RNN (10 points)\n",
        "\n",
        "Build RNN character-level language model of \"The girl from Samarcand\", by E. Hoffmann Price. You can use simple-RNN, GRU, LSTM or composite of any of these models.\n",
        "\n",
        "1. The beginning of this text must be trimmed. Start your text after \"\\*\\*\\* START OF THE PROJECT GUTENBERG EBOOK THE GIRL FROM SAMARCAND \\*\\*\\*\" and remove the title until the text starts with \"As her guest set...\".\n",
        "2. Delete the copyright information from Project Gutenberg at the bottom. Looking for \"\\*\\*\\* END OF THE PROJECT GUTENBERG EBOOK THE GIRL FROM SAMARCAND \\*\\*\\*\"\n",
        "3. All newlines should be replaced by a single space and any multiple space must be reduced into a single space.\n",
        "\n",
        "**Score**\n",
        "- 5 points will be giving to the code RNN that can be run on google colaboratory\n",
        "- Additional points will be given as follow\n",
        "   - 5 points will be giving to training with loss less than 1  \n",
        "   - 4 points will be giving to training with loss in [1, 2)    \n",
        "   - 3 points will be giving to training with loss in [2, 3)  \n",
        "   - 2 points will be giving to training with loss in [3, 5)  \n",
        "   - 1 points will be giving to training with loss greater than 5  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkzuGLVXZsFY"
      },
      "source": [
        "import mxnet as mx\n",
        "from mxnet import nd, autograd\n",
        "import numpy as np\n",
        "mx.random.seed(1)\n",
        "ctx = mx.cpu()"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAIoSQgjXuEY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "948f3414-2f23-4f12-ae19-0ff565138ad8"
      },
      "source": [
        "## DONOT CHANGE THIS LINE ##\n",
        "!wget https://www.gutenberg.org/ebooks/73463.txt.utf-8"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-27 03:48:55--  https://www.gutenberg.org/ebooks/73463.txt.utf-8\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: http://www.gutenberg.org/cache/epub/73463/pg73463.txt [following]\n",
            "--2024-04-27 03:48:56--  http://www.gutenberg.org/cache/epub/73463/pg73463.txt\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.gutenberg.org/cache/epub/73463/pg73463.txt [following]\n",
            "--2024-04-27 03:48:56--  https://www.gutenberg.org/cache/epub/73463/pg73463.txt\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49918 (49K) [text/plain]\n",
            "Saving to: ‘73463.txt.utf-8.2’\n",
            "\n",
            "73463.txt.utf-8.2   100%[===================>]  48.75K   144KB/s    in 0.3s    \n",
            "\n",
            "2024-04-27 03:48:57 (144 KB/s) - ‘73463.txt.utf-8.2’ saved [49918/49918]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Please preprocess data according to the above instruction using re ###\n",
        "with open(\"73463.txt.utf-8\") as f:\n",
        "    gbook = f.read()\n",
        "len(gbook)"
      ],
      "metadata": {
        "id": "ZYqWIJQ6m62l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b281a72-117a-4db0-a309-9cd6c9c67fb0"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48727"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(gbook[:1217])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DUkYRdzUjuB",
        "outputId": "b08e0e73-b5c6-4fba-ec71-dbb7023c6202"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿The Project Gutenberg eBook of The girl from Samarcand\n",
            "    \n",
            "This ebook is for the use of anyone anywhere in the United States and\n",
            "most other parts of the world at no cost and with almost no restrictions\n",
            "whatsoever. You may copy it, give it away or re-use it under the terms\n",
            "of the Project Gutenberg License included with this ebook or online\n",
            "at www.gutenberg.org. If you are not located in the United States,\n",
            "you will have to check the laws of the country where you are located\n",
            "before using this eBook.\n",
            "\n",
            "Title: The girl from Samarcand\n",
            "\n",
            "Author: E. Hoffman Price\n",
            "\n",
            "Release date: April 25, 2024 [eBook #73463]\n",
            "\n",
            "Language: English\n",
            "\n",
            "Original publication: Indianapolis, IN: Popular Fiction Publishing Company, 1929\n",
            "\n",
            "Credits: Greg Weeks, Mary Meehan and the Online Distributed Proofreading Team at http://www.pgdp.net\n",
            "\n",
            "\n",
            "*** START OF THE PROJECT GUTENBERG EBOOK THE GIRL FROM SAMARCAND ***\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                        The GIRL from SAMARCAND\n",
            "\n",
            "                         By E. HOFFMANN PRICE\n",
            "\n",
            "           [Transcriber's Note: This etext was produced from\n",
            "                         Weird Tales May 1929.\n",
            "         Extensive research did not uncover any evidence that\n",
            "         the U.S. copyright on this publication was renewed.]\n",
            "\n",
            "\n",
            "A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(gbook[-18496:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keO5IXe7UpnW",
        "outputId": "19943caa-9233-45a4-d685-1abd9327d920"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*** END OF THE PROJECT GUTENBERG EBOOK THE GIRL FROM SAMARCAND ***\n",
            "\n",
            "\n",
            "    \n",
            "\n",
            "Updated editions will replace the previous one—the old editions will\n",
            "be renamed.\n",
            "\n",
            "Creating the works from print editions not protected by U.S. copyright\n",
            "law means that no one owns a United States copyright in these works,\n",
            "so the Foundation (and you!) can copy and distribute it in the United\n",
            "States without permission and without paying copyright\n",
            "royalties. Special rules, set forth in the General Terms of Use part\n",
            "of this license, apply to copying and distributing Project\n",
            "Gutenberg™ electronic works to protect the PROJECT GUTENBERG™\n",
            "concept and trademark. Project Gutenberg is a registered trademark,\n",
            "and may not be used if you charge for an eBook, except by following\n",
            "the terms of the trademark license, including paying royalties for use\n",
            "of the Project Gutenberg trademark. If you do not charge anything for\n",
            "copies of this eBook, complying with the trademark license is very\n",
            "easy. You may use this eBook for nearly any purpose such as creation\n",
            "of derivative works, reports, performances and research. Project\n",
            "Gutenberg eBooks may be modified and printed and given away—you may\n",
            "do practically ANYTHING in the United States with eBooks not protected\n",
            "by U.S. copyright law. Redistribution is subject to the trademark\n",
            "license, especially commercial redistribution.\n",
            "\n",
            "\n",
            "START: FULL LICENSE\n",
            "\n",
            "THE FULL PROJECT GUTENBERG LICENSE\n",
            "\n",
            "PLEASE READ THIS BEFORE YOU DISTRIBUTE OR USE THIS WORK\n",
            "\n",
            "To protect the Project Gutenberg™ mission of promoting the free\n",
            "distribution of electronic works, by using or distributing this work\n",
            "(or any other work associated in any way with the phrase “Project\n",
            "Gutenberg”), you agree to comply with all the terms of the Full\n",
            "Project Gutenberg™ License available with this file or online at\n",
            "www.gutenberg.org/license.\n",
            "\n",
            "Section 1. General Terms of Use and Redistributing Project Gutenberg™\n",
            "electronic works\n",
            "\n",
            "1.A. By reading or using any part of this Project Gutenberg™\n",
            "electronic work, you indicate that you have read, understand, agree to\n",
            "and accept all the terms of this license and intellectual property\n",
            "(trademark/copyright) agreement. If you do not agree to abide by all\n",
            "the terms of this agreement, you must cease using and return or\n",
            "destroy all copies of Project Gutenberg™ electronic works in your\n",
            "possession. If you paid a fee for obtaining a copy of or access to a\n",
            "Project Gutenberg™ electronic work and you do not agree to be bound\n",
            "by the terms of this agreement, you may obtain a refund from the person\n",
            "or entity to whom you paid the fee as set forth in paragraph 1.E.8.\n",
            "\n",
            "1.B. “Project Gutenberg” is a registered trademark. It may only be\n",
            "used on or associated in any way with an electronic work by people who\n",
            "agree to be bound by the terms of this agreement. There are a few\n",
            "things that you can do with most Project Gutenberg™ electronic works\n",
            "even without complying with the full terms of this agreement. See\n",
            "paragraph 1.C below. There are a lot of things you can do with Project\n",
            "Gutenberg™ electronic works if you follow the terms of this\n",
            "agreement and help preserve free future access to Project Gutenberg™\n",
            "electronic works. See paragraph 1.E below.\n",
            "\n",
            "1.C. The Project Gutenberg Literary Archive Foundation (“the\n",
            "Foundation” or PGLAF), owns a compilation copyright in the collection\n",
            "of Project Gutenberg™ electronic works. Nearly all the individual\n",
            "works in the collection are in the public domain in the United\n",
            "States. If an individual work is unprotected by copyright law in the\n",
            "United States and you are located in the United States, we do not\n",
            "claim a right to prevent you from copying, distributing, performing,\n",
            "displaying or creating derivative works based on the work as long as\n",
            "all references to Project Gutenberg are removed. Of course, we hope\n",
            "that you will support the Project Gutenberg™ mission of promoting\n",
            "free access to electronic works by freely sharing Project Gutenberg™\n",
            "works in compliance with the terms of this agreement for keeping the\n",
            "Project Gutenberg™ name associated with the work. You can easily\n",
            "comply with the terms of this agreement by keeping this work in the\n",
            "same format with its attached full Project Gutenberg™ License when\n",
            "you share it without charge with others.\n",
            "\n",
            "1.D. The copyright laws of the place where you are located also govern\n",
            "what you can do with this work. Copyright laws in most countries are\n",
            "in a constant state of change. If you are outside the United States,\n",
            "check the laws of your country in addition to the terms of this\n",
            "agreement before downloading, copying, displaying, performing,\n",
            "distributing or creating derivative works based on this work or any\n",
            "other Project Gutenberg™ work. The Foundation makes no\n",
            "representations concerning the copyright status of any work in any\n",
            "country other than the United States.\n",
            "\n",
            "1.E. Unless you have removed all references to Project Gutenberg:\n",
            "\n",
            "1.E.1. The following sentence, with active links to, or other\n",
            "immediate access to, the full Project Gutenberg™ License must appear\n",
            "prominently whenever any copy of a Project Gutenberg™ work (any work\n",
            "on which the phrase “Project Gutenberg” appears, or with which the\n",
            "phrase “Project Gutenberg” is associated) is accessed, displayed,\n",
            "performed, viewed, copied or distributed:\n",
            "\n",
            "    This eBook is for the use of anyone anywhere in the United States and most\n",
            "    other parts of the world at no cost and with almost no restrictions\n",
            "    whatsoever. You may copy it, give it away or re-use it under the terms\n",
            "    of the Project Gutenberg License included with this eBook or online\n",
            "    at www.gutenberg.org. If you\n",
            "    are not located in the United States, you will have to check the laws\n",
            "    of the country where you are located before using this eBook.\n",
            "  \n",
            "1.E.2. If an individual Project Gutenberg™ electronic work is\n",
            "derived from texts not protected by U.S. copyright law (does not\n",
            "contain a notice indicating that it is posted with permission of the\n",
            "copyright holder), the work can be copied and distributed to anyone in\n",
            "the United States without paying any fees or charges. If you are\n",
            "redistributing or providing access to a work with the phrase “Project\n",
            "Gutenberg” associated with or appearing on the work, you must comply\n",
            "either with the requirements of paragraphs 1.E.1 through 1.E.7 or\n",
            "obtain permission for the use of the work and the Project Gutenberg™\n",
            "trademark as set forth in paragraphs 1.E.8 or 1.E.9.\n",
            "\n",
            "1.E.3. If an individual Project Gutenberg™ electronic work is posted\n",
            "with the permission of the copyright holder, your use and distribution\n",
            "must comply with both paragraphs 1.E.1 through 1.E.7 and any\n",
            "additional terms imposed by the copyright holder. Additional terms\n",
            "will be linked to the Project Gutenberg™ License for all works\n",
            "posted with the permission of the copyright holder found at the\n",
            "beginning of this work.\n",
            "\n",
            "1.E.4. Do not unlink or detach or remove the full Project Gutenberg™\n",
            "License terms from this work, or any files containing a part of this\n",
            "work or any other work associated with Project Gutenberg™.\n",
            "\n",
            "1.E.5. Do not copy, display, perform, distribute or redistribute this\n",
            "electronic work, or any part of this electronic work, without\n",
            "prominently displaying the sentence set forth in paragraph 1.E.1 with\n",
            "active links or immediate access to the full terms of the Project\n",
            "Gutenberg™ License.\n",
            "\n",
            "1.E.6. You may convert to and distribute this work in any binary,\n",
            "compressed, marked up, nonproprietary or proprietary form, including\n",
            "any word processing or hypertext form. However, if you provide access\n",
            "to or distribute copies of a Project Gutenberg™ work in a format\n",
            "other than “Plain Vanilla ASCII” or other format used in the official\n",
            "version posted on the official Project Gutenberg™ website\n",
            "(www.gutenberg.org), you must, at no additional cost, fee or expense\n",
            "to the user, provide a copy, a means of exporting a copy, or a means\n",
            "of obtaining a copy upon request, of the work in its original “Plain\n",
            "Vanilla ASCII” or other form. Any alternate format must include the\n",
            "full Project Gutenberg™ License as specified in paragraph 1.E.1.\n",
            "\n",
            "1.E.7. Do not charge a fee for access to, viewing, displaying,\n",
            "performing, copying or distributing any Project Gutenberg™ works\n",
            "unless you comply with paragraph 1.E.8 or 1.E.9.\n",
            "\n",
            "1.E.8. You may charge a reasonable fee for copies of or providing\n",
            "access to or distributing Project Gutenberg™ electronic works\n",
            "provided that:\n",
            "\n",
            "    • You pay a royalty fee of 20% of the gross profits you derive from\n",
            "        the use of Project Gutenberg™ works calculated using the method\n",
            "        you already use to calculate your applicable taxes. The fee is owed\n",
            "        to the owner of the Project Gutenberg™ trademark, but he has\n",
            "        agreed to donate royalties under this paragraph to the Project\n",
            "        Gutenberg Literary Archive Foundation. Royalty payments must be paid\n",
            "        within 60 days following each date on which you prepare (or are\n",
            "        legally required to prepare) your periodic tax returns. Royalty\n",
            "        payments should be clearly marked as such and sent to the Project\n",
            "        Gutenberg Literary Archive Foundation at the address specified in\n",
            "        Section 4, “Information about donations to the Project Gutenberg\n",
            "        Literary Archive Foundation.”\n",
            "    \n",
            "    • You provide a full refund of any money paid by a user who notifies\n",
            "        you in writing (or by e-mail) within 30 days of receipt that s/he\n",
            "        does not agree to the terms of the full Project Gutenberg™\n",
            "        License. You must require such a user to return or destroy all\n",
            "        copies of the works possessed in a physical medium and discontinue\n",
            "        all use of and all access to other copies of Project Gutenberg™\n",
            "        works.\n",
            "    \n",
            "    • You provide, in accordance with paragraph 1.F.3, a full refund of\n",
            "        any money paid for a work or a replacement copy, if a defect in the\n",
            "        electronic work is discovered and reported to you within 90 days of\n",
            "        receipt of the work.\n",
            "    \n",
            "    • You comply with all other terms of this agreement for free\n",
            "        distribution of Project Gutenberg™ works.\n",
            "    \n",
            "\n",
            "1.E.9. If you wish to charge a fee or distribute a Project\n",
            "Gutenberg™ electronic work or group of works on different terms than\n",
            "are set forth in this agreement, you must obtain permission in writing\n",
            "from the Project Gutenberg Literary Archive Foundation, the manager of\n",
            "the Project Gutenberg™ trademark. Contact the Foundation as set\n",
            "forth in Section 3 below.\n",
            "\n",
            "1.F.\n",
            "\n",
            "1.F.1. Project Gutenberg volunteers and employees expend considerable\n",
            "effort to identify, do copyright research on, transcribe and proofread\n",
            "works not protected by U.S. copyright law in creating the Project\n",
            "Gutenberg™ collection. Despite these efforts, Project Gutenberg™\n",
            "electronic works, and the medium on which they may be stored, may\n",
            "contain “Defects,” such as, but not limited to, incomplete, inaccurate\n",
            "or corrupt data, transcription errors, a copyright or other\n",
            "intellectual property infringement, a defective or damaged disk or\n",
            "other medium, a computer virus, or computer codes that damage or\n",
            "cannot be read by your equipment.\n",
            "\n",
            "1.F.2. LIMITED WARRANTY, DISCLAIMER OF DAMAGES - Except for the “Right\n",
            "of Replacement or Refund” described in paragraph 1.F.3, the Project\n",
            "Gutenberg Literary Archive Foundation, the owner of the Project\n",
            "Gutenberg™ trademark, and any other party distributing a Project\n",
            "Gutenberg™ electronic work under this agreement, disclaim all\n",
            "liability to you for damages, costs and expenses, including legal\n",
            "fees. YOU AGREE THAT YOU HAVE NO REMEDIES FOR NEGLIGENCE, STRICT\n",
            "LIABILITY, BREACH OF WARRANTY OR BREACH OF CONTRACT EXCEPT THOSE\n",
            "PROVIDED IN PARAGRAPH 1.F.3. YOU AGREE THAT THE FOUNDATION, THE\n",
            "TRADEMARK OWNER, AND ANY DISTRIBUTOR UNDER THIS AGREEMENT WILL NOT BE\n",
            "LIABLE TO YOU FOR ACTUAL, DIRECT, INDIRECT, CONSEQUENTIAL, PUNITIVE OR\n",
            "INCIDENTAL DAMAGES EVEN IF YOU GIVE NOTICE OF THE POSSIBILITY OF SUCH\n",
            "DAMAGE.\n",
            "\n",
            "1.F.3. LIMITED RIGHT OF REPLACEMENT OR REFUND - If you discover a\n",
            "defect in this electronic work within 90 days of receiving it, you can\n",
            "receive a refund of the money (if any) you paid for it by sending a\n",
            "written explanation to the person you received the work from. If you\n",
            "received the work on a physical medium, you must return the medium\n",
            "with your written explanation. The person or entity that provided you\n",
            "with the defective work may elect to provide a replacement copy in\n",
            "lieu of a refund. If you received the work electronically, the person\n",
            "or entity providing it to you may choose to give you a second\n",
            "opportunity to receive the work electronically in lieu of a refund. If\n",
            "the second copy is also defective, you may demand a refund in writing\n",
            "without further opportunities to fix the problem.\n",
            "\n",
            "1.F.4. Except for the limited right of replacement or refund set forth\n",
            "in paragraph 1.F.3, this work is provided to you ‘AS-IS’, WITH NO\n",
            "OTHER WARRANTIES OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT\n",
            "LIMITED TO WARRANTIES OF MERCHANTABILITY OR FITNESS FOR ANY PURPOSE.\n",
            "\n",
            "1.F.5. Some states do not allow disclaimers of certain implied\n",
            "warranties or the exclusion or limitation of certain types of\n",
            "damages. If any disclaimer or limitation set forth in this agreement\n",
            "violates the law of the state applicable to this agreement, the\n",
            "agreement shall be interpreted to make the maximum disclaimer or\n",
            "limitation permitted by the applicable state law. The invalidity or\n",
            "unenforceability of any provision of this agreement shall not void the\n",
            "remaining provisions.\n",
            "\n",
            "1.F.6. INDEMNITY - You agree to indemnify and hold the Foundation, the\n",
            "trademark owner, any agent or employee of the Foundation, anyone\n",
            "providing copies of Project Gutenberg™ electronic works in\n",
            "accordance with this agreement, and any volunteers associated with the\n",
            "production, promotion and distribution of Project Gutenberg™\n",
            "electronic works, harmless from all liability, costs and expenses,\n",
            "including legal fees, that arise directly or indirectly from any of\n",
            "the following which you do or cause to occur: (a) distribution of this\n",
            "or any Project Gutenberg™ work, (b) alteration, modification, or\n",
            "additions or deletions to any Project Gutenberg™ work, and (c) any\n",
            "Defect you cause.\n",
            "\n",
            "Section 2. Information about the Mission of Project Gutenberg™\n",
            "\n",
            "Project Gutenberg™ is synonymous with the free distribution of\n",
            "electronic works in formats readable by the widest variety of\n",
            "computers including obsolete, old, middle-aged and new computers. It\n",
            "exists because of the efforts of hundreds of volunteers and donations\n",
            "from people in all walks of life.\n",
            "\n",
            "Volunteers and financial support to provide volunteers with the\n",
            "assistance they need are critical to reaching Project Gutenberg™’s\n",
            "goals and ensuring that the Project Gutenberg™ collection will\n",
            "remain freely available for generations to come. In 2001, the Project\n",
            "Gutenberg Literary Archive Foundation was created to provide a secure\n",
            "and permanent future for Project Gutenberg™ and future\n",
            "generations. To learn more about the Project Gutenberg Literary\n",
            "Archive Foundation and how your efforts and donations can help, see\n",
            "Sections 3 and 4 and the Foundation information page at www.gutenberg.org.\n",
            "\n",
            "Section 3. Information about the Project Gutenberg Literary Archive Foundation\n",
            "\n",
            "The Project Gutenberg Literary Archive Foundation is a non-profit\n",
            "501(c)(3) educational corporation organized under the laws of the\n",
            "state of Mississippi and granted tax exempt status by the Internal\n",
            "Revenue Service. The Foundation’s EIN or federal tax identification\n",
            "number is 64-6221541. Contributions to the Project Gutenberg Literary\n",
            "Archive Foundation are tax deductible to the full extent permitted by\n",
            "U.S. federal laws and your state’s laws.\n",
            "\n",
            "The Foundation’s business office is located at 809 North 1500 West,\n",
            "Salt Lake City, UT 84116, (801) 596-1887. Email contact links and up\n",
            "to date contact information can be found at the Foundation’s website\n",
            "and official page at www.gutenberg.org/contact\n",
            "\n",
            "Section 4. Information about Donations to the Project Gutenberg\n",
            "Literary Archive Foundation\n",
            "\n",
            "Project Gutenberg™ depends upon and cannot survive without widespread\n",
            "public support and donations to carry out its mission of\n",
            "increasing the number of public domain and licensed works that can be\n",
            "freely distributed in machine-readable form accessible by the widest\n",
            "array of equipment including outdated equipment. Many small donations\n",
            "($1 to $5,000) are particularly important to maintaining tax exempt\n",
            "status with the IRS.\n",
            "\n",
            "The Foundation is committed to complying with the laws regulating\n",
            "charities and charitable donations in all 50 states of the United\n",
            "States. Compliance requirements are not uniform and it takes a\n",
            "considerable effort, much paperwork and many fees to meet and keep up\n",
            "with these requirements. We do not solicit donations in locations\n",
            "where we have not received written confirmation of compliance. To SEND\n",
            "DONATIONS or determine the status of compliance for any particular state\n",
            "visit www.gutenberg.org/donate.\n",
            "\n",
            "While we cannot and do not solicit contributions from states where we\n",
            "have not met the solicitation requirements, we know of no prohibition\n",
            "against accepting unsolicited donations from donors in such states who\n",
            "approach us with offers to donate.\n",
            "\n",
            "International donations are gratefully accepted, but we cannot make\n",
            "any statements concerning tax treatment of donations received from\n",
            "outside the United States. U.S. laws alone swamp our small staff.\n",
            "\n",
            "Please check the Project Gutenberg web pages for current donation\n",
            "methods and addresses. Donations are accepted in a number of other\n",
            "ways including checks, online payments and credit card donations. To\n",
            "donate, please visit: www.gutenberg.org/donate.\n",
            "\n",
            "Section 5. General Information About Project Gutenberg™ electronic works\n",
            "\n",
            "Professor Michael S. Hart was the originator of the Project\n",
            "Gutenberg™ concept of a library of electronic works that could be\n",
            "freely shared with anyone. For forty years, he produced and\n",
            "distributed Project Gutenberg™ eBooks with only a loose network of\n",
            "volunteer support.\n",
            "\n",
            "Project Gutenberg™ eBooks are often created from several printed\n",
            "editions, all of which are confirmed as not protected by copyright in\n",
            "the U.S. unless a copyright notice is included. Thus, we do not\n",
            "necessarily keep eBooks in compliance with any particular paper\n",
            "edition.\n",
            "\n",
            "Most people start at our website which has the main PG search\n",
            "facility: www.gutenberg.org.\n",
            "\n",
            "This website includes information about Project Gutenberg™,\n",
            "including how to make donations to the Project Gutenberg Literary\n",
            "Archive Foundation, how to help produce our new eBooks, and how to\n",
            "subscribe to our email newsletter to hear about new eBooks.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-process data\n",
        "gbook = gbook[:-18496] # Remove from the back\n",
        "gbook = gbook[1216:]  # Remove the first part\n",
        "print(gbook[:100])\n",
        "print(gbook[-100:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jlU0dC7Urn6",
        "outputId": "9b60a892-e587-4ec0-f458-b385a525da2b"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As her guest set the dainty bone china cup on the onyx-topped, teak\n",
            "tabouret and sank back among the\n",
            "cand gleamed for another moment in\n",
            "the moonlight, then sweltered in the red glow of the mosque lamp.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(gbook[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtfiKF2AUvGD",
        "outputId": "9bb316bb-8c69-415d-8c11-baae7e8a018c"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As her guest set the dainty bone china cup on the onyx-topped, teak\n",
            "tabouret and sank back among the embroidered cushions, Diane knew to\n",
            "the syllable the words which were to filter forth with the next breath\n",
            "of smoke; for three years as Hammersmith Clarke's wife had convinced\n",
            "her that that remark was inevitable.\n",
            "\n",
            "\"My dear, where did you ever get those perfectly gorgeous rugs?\"\n",
            "\n",
            "And Diane, true to form, smiled ever so faintly, and luxuriated in\n",
            "the suspicion of a yawn: the ennui of an odalisk hardened to the\n",
            "magnificence of a seraglio carpeted with an ancient Feraghan rug, and\n",
            "hung with silken witcheries from the looms of Kashan. Diane saw the\n",
            "wonder permeate her friend's soul and heard it surge into words.\n",
            "\n",
            "\"The rugs? Why--well, I married them along with Ham, you might say.\n",
            "Yes, they are rather pretty, aren't they? But they're an awful pest at\n",
            "times----\"\n",
            "\n",
            "\"Naturally,\" agreed Louise, who lived in a loft in the Pontalba\n",
            "Building, where she could look down into the Plaza where Jackson\n",
            "rei\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all characters to lower cases\n",
        "gbook = gbook.lower()"
      ],
      "metadata": {
        "id": "pSs1JuuUUy3M"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert newline to space\n",
        "r_pattern = re.compile(\"[\\n\\t\\r]\")\n",
        "gbook = r_pattern.sub(\" \", gbook)"
      ],
      "metadata": {
        "id": "Pr6maJTZU0zz"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## DO NOT CHANGE THIS BLOCK ##\n",
        "character_list = list(set(gbook))\n",
        "vocab_size = len(character_list)\n",
        "print(character_list)\n",
        "print(\"Length of vocab: %s\" % vocab_size)\n",
        "\n",
        "# Store them in dictionary\n",
        "character_dict = {}\n",
        "for e, char in enumerate(character_list):\n",
        "    character_dict[char] = e\n",
        "print(character_dict)\n",
        "gbook_numerical = [character_dict[char] for char in gbook]\n",
        "\n",
        "#  Check that the length is right\n",
        "print(\"The number of characters in this book is \", len(gbook))\n",
        "print(gbook_numerical[:20])\n",
        "print(\"\".join([character_list[idx] for idx in gbook_numerical[:39]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvMkI_tzv_aq",
        "outputId": "eea95aa8-49c6-45a4-93c2-ea366a8cef6c"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[', 'k', '-', 'è', '!', 'f', \"'\", ':', ' ', 'e', 'v', '\"', 'h', 'y', 'j', '_', 'l', ';', '*', ',', 'q', 's', ']', 'b', 'z', 'à', 'a', 'n', 'i', 'p', 'ê', 'r', 'é', 'm', 't', 'u', 'o', '.', '?', 'w', 'g', 'd', 'c', 'x', '3']\n",
            "Length of vocab: 45\n",
            "{'[': 0, 'k': 1, '-': 2, 'è': 3, '!': 4, 'f': 5, \"'\": 6, ':': 7, ' ': 8, 'e': 9, 'v': 10, '\"': 11, 'h': 12, 'y': 13, 'j': 14, '_': 15, 'l': 16, ';': 17, '*': 18, ',': 19, 'q': 20, 's': 21, ']': 22, 'b': 23, 'z': 24, 'à': 25, 'a': 26, 'n': 27, 'i': 28, 'p': 29, 'ê': 30, 'r': 31, 'é': 32, 'm': 33, 't': 34, 'u': 35, 'o': 36, '.': 37, '?': 38, 'w': 39, 'g': 40, 'd': 41, 'c': 42, 'x': 43, '3': 44}\n",
            "The number of characters in this book is  29015\n",
            "[26, 21, 8, 12, 9, 31, 8, 40, 35, 9, 21, 34, 8, 21, 9, 34, 8, 34, 12, 9]\n",
            "as her guest set the dainty bone china \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## DO NOT CHANGE THIS BLOCK ##\n",
        "def one_hots(numerical_list, vocab_size=vocab_size):\n",
        "    result = nd.zeros((len(numerical_list), vocab_size), ctx=ctx)\n",
        "    for i, idx in enumerate(numerical_list):\n",
        "        result[i, idx] = 1.0\n",
        "    return result\n",
        "def textify(embedding):\n",
        "    result = \"\"\n",
        "    indices = nd.argmax(embedding, axis=1).asnumpy()\n",
        "    for idx in indices:\n",
        "        result += character_list[int(idx)]\n",
        "    return result\n",
        "\n",
        "textify(one_hots(gbook_numerical[0:40]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "JtK9MsX-nXh6",
        "outputId": "0c67549d-5f33-4433-fd30-6d1329bbdcb3"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'as her guest set the dainty bone china c'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the remaining division of the total characters and seq_length\n",
        "len(gbook)//32, len(gbook)%32"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbYQ2BR8U4rw",
        "outputId": "387ced3f-7887-4ae3-a832-f72505e52fef"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(906, 23)"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 32\n",
        "# Need to subtract the number of characters to align with seq_length\n",
        "num_samples = (len(gbook_numerical) - 23) // seq_length\n",
        "dataset = one_hots(gbook_numerical[:seq_length*num_samples]).reshape((num_samples, seq_length, vocab_size))\n",
        "textify(dataset[0])"
      ],
      "metadata": {
        "id": "8MNSKAUSuhiL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "ce4d7fac-2f3b-4834-cdb5-fd3687e836c8"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'as her guest set the dainty bone'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "print('# of sequences in dataset: ', len(dataset))\n",
        "num_batches = len(dataset) // batch_size\n",
        "print('# of batches: ', num_batches)\n",
        "train_data = dataset[:num_batches*batch_size].reshape((batch_size, num_batches, seq_length, vocab_size))\n",
        "# swap batch_size and seq_length axis to make later access easier\n",
        "train_data = nd.swapaxes(train_data, 0, 1)\n",
        "train_data = nd.swapaxes(train_data, 1, 2)\n",
        "print('Shape of data set: ', train_data.shape)\n",
        "\n",
        "for i in range(3):\n",
        "    print(\"***Batch %s:***\\n %s \\n %s \\n\\n\" % (i, textify(train_data[i, :, 0]), textify(train_data[i, :, 1])))"
      ],
      "metadata": {
        "id": "Kk5zQfvouhbg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba374414-29f5-4a08-e1a3-4bcb8703db1b"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of sequences in dataset:  906\n",
            "# of batches:  28\n",
            "Shape of data set:  (28, 32, 32, 45)\n",
            "***Batch 0:***\n",
            " as her guest set the dainty bone \n",
            " who lived in a loft in the ponta \n",
            "\n",
            "\n",
            "***Batch 1:***\n",
            "  china cup on the onyx-topped, t \n",
            " lba building, where she could lo \n",
            "\n",
            "\n",
            "***Batch 2:***\n",
            " eak tabouret and sank back among \n",
            " ok down into the plaza where jac \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(labels), batch_size*num_batches*seq_length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNyq94SYVBjD",
        "outputId": "e697481d-5223-42e7-fb23-87277e7c4e1b"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28992, 28672)"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 320 is removed to match the shape of the train_label\n",
        "labels = one_hots(gbook_numerical[1:seq_length*num_samples+1-320])\n",
        "train_label = labels.reshape((batch_size, num_batches, seq_length, vocab_size))\n",
        "train_label = nd.swapaxes(train_label, 0, 1)\n",
        "train_label = nd.swapaxes(train_label, 1, 2)\n",
        "print(train_label.shape)\n",
        "\n",
        "print(textify(train_data[10, :, 3]))\n",
        "print(textify(train_label[10, :, 3]))"
      ],
      "metadata": {
        "id": "dDiHt1PQn1A4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e049620-147f-46fa-8471-7b4804996a70"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(28, 32, 32, 45)\n",
            "ive with it day after day. see t\n",
            "ve with it day after day. see th\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vanila RNN"
      ],
      "metadata": {
        "id": "l8BsSfy83DeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_inputs, num_hidden, num_outputs = vocab_size, 256, vocab_size\n",
        "#  Weights connecting the inputs to the hidden layer\n",
        "Wxh = nd.random_normal(shape=(num_inputs,num_hidden), ctx=ctx) * 0.01\n",
        "#  Recurrent weights connecting the hidden layer across time steps\n",
        "Whh = nd.random_normal(shape=(num_hidden,num_hidden), ctx=ctx) * 0.01\n",
        "#  Bias vector for hidden layer\n",
        "bh = nd.random_normal(shape=num_hidden, ctx=ctx) * 0.01\n",
        "# Weights to the output nodes\n",
        "Why = nd.random_normal(shape=(num_hidden,num_outputs), ctx=ctx) * 0.01\n",
        "by = nd.random_normal(shape=num_outputs, ctx=ctx) * 0.01\n",
        "\n",
        "params = [Wxh, Whh, bh, Why, by]\n",
        "for param in params:\n",
        "    param.attach_grad()\n",
        "def softmax(y_linear, temperature=1.0):\n",
        "    lin = (y_linear-nd.max(y_linear, axis=1).reshape((-1,1))) / temperature # shift each row of y_linear by its max\n",
        "    exp = nd.exp(lin)\n",
        "    partition =nd.sum(exp, axis=1).reshape((-1,1))\n",
        "    return exp / partition\n",
        "# With a temperature of 1, get back some set of probabilities\n",
        "softmax(nd.array([[1, -1], [-1, 1]]), temperature=1.0)\n",
        "# With a high temperature, get more entropic (*noisier*) probabilities\n",
        "softmax(nd.array([[1,-1],[-1,1]]), temperature=1000.0)\n",
        "# With low temperatures to produce sharp probabilities\n",
        "softmax(nd.array([[10,-10],[-10,10]]), temperature=.1)\n",
        "def simple_rnn(inputs, state, temperature=1.0):\n",
        "    outputs = []\n",
        "    h = state\n",
        "    for X in inputs:\n",
        "        h_linear = nd.dot(X, Wxh) + nd.dot(h, Whh) + bh\n",
        "        h = nd.tanh(h_linear)\n",
        "        yhat_linear = nd.dot(h, Why) + by\n",
        "        yhat = softmax(yhat_linear, temperature=temperature)\n",
        "        outputs.append(yhat)\n",
        "    return (outputs, h)"
      ],
      "metadata": {
        "id": "WO6T2ZuoblD1"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy(yhat, y):\n",
        "    return - nd.mean(nd.sum(y * nd.log(yhat), axis=0, exclude=True))\n",
        "cross_entropy(nd.array([[.2,.5,.3], [.2,.5,.3]]), nd.array([[1.,0,0], [0, 1.,0]]))\n",
        "# Averaging the loss over the sequence\n",
        "def average_ce_loss(outputs, labels):\n",
        "    assert(len(outputs) == len(labels))\n",
        "    total_loss = 0.\n",
        "    for (output, label) in zip(outputs,labels):\n",
        "        total_loss = total_loss + cross_entropy(output, label)\n",
        "    return total_loss / len(outputs)\n",
        "# Optimizer\n",
        "def SGD(params, lr):\n",
        "    for param in params:\n",
        "        param[:] = param - lr * param.grad\n",
        "\n",
        "def sample(prefix, num_chars, temperature=1.0):\n",
        "    # Initialize the string that we'll return to the supplied prefix\n",
        "    string = prefix\n",
        "    # Prepare the prefix as a sequence of one-hots for ingestion by RNN\n",
        "    prefix_numerical = [character_dict[char] for char in prefix]\n",
        "    input_sequence = one_hots(prefix_numerical)\n",
        "    # Set the initial state of the hidden representation ($h_0$) to the zero vector\n",
        "    sample_state = nd.zeros(shape=(1, num_hidden), ctx=ctx)\n",
        "    for i in range(num_chars):\n",
        "        outputs, sample_state = simple_rnn(input_sequence, sample_state, temperature=temperature)\n",
        "        choice = np.random.choice(vocab_size, p=outputs[-1][0].asnumpy())\n",
        "        string += character_list[choice]\n",
        "        input_sequence = one_hots([choice])\n",
        "    return string"
      ],
      "metadata": {
        "id": "OEwdpKSkbyoX"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs, moving_loss, learning_rate = 20, 0.01, 0.05\n",
        "for e in range(epochs):\n",
        "    # Attenuate the learning rate by a factor of 2 every 10 epochs.\n",
        "    if ((e+1) % 10 == 0):\n",
        "        learning_rate = learning_rate / 2.0\n",
        "    state = nd.zeros(shape=(batch_size, num_hidden), ctx=ctx)\n",
        "    for i in range(num_batches):\n",
        "        data_one_hot = train_data[i]\n",
        "        label_one_hot = train_label[i]\n",
        "        with autograd.record():\n",
        "            outputs, state = simple_rnn(data_one_hot, state)\n",
        "            loss = average_ce_loss(outputs, label_one_hot)\n",
        "            loss.backward()\n",
        "        SGD(params, learning_rate)\n",
        "        #  Keep a moving average of the losses\n",
        "        if (i == 0) and (e == 0):\n",
        "            moving_loss = np.mean(loss.asnumpy()[0])\n",
        "        else:\n",
        "            moving_loss = .99 * moving_loss + .01 * np.mean(loss.asnumpy()[0])\n",
        "    print(\"Epoch %d. Loss: %9.4F\" % (e+1, moving_loss))\n",
        "    print(sample(\"i knew that\", 8, temperature=.1))\n",
        "    print(sample(\"clarke smiled incredulously\", 8, temperature=.1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af6Oats1ekiz",
        "outputId": "cf99f87f-3b8c-4c37-a5cf-34962c24f6a1"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1. Loss:    0.4498\n",
            "i knew that i caplo\n",
            "clarke smiled incredulously: which \n",
            "Epoch 2. Loss:    0.4391\n",
            "i knew that i palle\n",
            "clarke smiled incredulously: which \n",
            "Epoch 3. Loss:    0.4302\n",
            "i knew that i caplo\n",
            "clarke smiled incredulously: which \n",
            "Epoch 4. Loss:    0.4226\n",
            "i knew that i palle\n",
            "clarke smiled incredulously: which \n",
            "Epoch 5. Loss:    0.4161\n",
            "i knew that i palle\n",
            "clarke smiled incredulously: which \n",
            "Epoch 6. Loss:    0.4106\n",
            "i knew that i palle\n",
            "clarke smiled incredulously: which \n",
            "Epoch 7. Loss:    0.4059\n",
            "i knew that i palle\n",
            "clarke smiled incredulously: which \n",
            "Epoch 8. Loss:    0.4017\n",
            "i knew that i palle\n",
            "clarke smiled incredulously: which \n",
            "Epoch 9. Loss:    0.3982\n",
            "i knew that i palle\n",
            "clarke smiled incredulously: which \n",
            "Epoch 10. Loss:    0.3948\n",
            "i knew that i palle\n",
            "clarke smiled incredulously: which \n",
            "Epoch 11. Loss:    0.3907\n",
            "i knew that i palle\n",
            "clarke smiled incredulously: which \n",
            "Epoch 12. Loss:    0.3869\n",
            "i knew that i palle\n",
            "clarke smiled incredulously: which \n",
            "Epoch 13. Loss:    0.3836\n",
            "i knew that i palle\n",
            "clarke smiled incredulously: which \n",
            "Epoch 14. Loss:    0.3807\n",
            "i knew that i palle\n",
            "clarke smiled incredulously: which \n",
            "Epoch 15. Loss:    0.3781\n",
            "i knew that i palle\n",
            "clarke smiled incredulously: which \n",
            "Epoch 16. Loss:    0.3757\n",
            "i knew that i palle\n",
            "clarke smiled incredulously: which \n",
            "Epoch 17. Loss:    0.3736\n",
            "i knew that i palle\n",
            "clarke smiled incredulously: which \n",
            "Epoch 18. Loss:    0.3717\n",
            "i knew that i palle\n",
            "clarke smiled incredulously: which \n",
            "Epoch 19. Loss:    0.3699\n",
            "i knew that i palle\n",
            "clarke smiled incredulously: which \n",
            "Epoch 20. Loss:    0.3681\n",
            "i knew that i palle\n",
            "clarke smiled incredulously: which \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM"
      ],
      "metadata": {
        "id": "fUqxaEJUVTk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize parameters\n",
        "num_inputs = vocab_size\n",
        "num_hidden = 256\n",
        "num_outputs = vocab_size\n",
        "\n",
        "########################\n",
        "#  Weights connecting the inputs to the hidden layer\n",
        "########################\n",
        "Wxg = nd.random_normal(shape=(num_inputs,num_hidden), ctx=ctx) * .01\n",
        "Wxi = nd.random_normal(shape=(num_inputs,num_hidden), ctx=ctx) * .01\n",
        "Wxf = nd.random_normal(shape=(num_inputs,num_hidden), ctx=ctx) * .01\n",
        "Wxo = nd.random_normal(shape=(num_inputs,num_hidden), ctx=ctx) * .01\n",
        "\n",
        "########################\n",
        "#  Recurrent weights connecting the hidden layer across time steps\n",
        "########################\n",
        "Whg = nd.random_normal(shape=(num_hidden,num_hidden), ctx=ctx)* .01\n",
        "Whi = nd.random_normal(shape=(num_hidden,num_hidden), ctx=ctx)* .01\n",
        "Whf = nd.random_normal(shape=(num_hidden,num_hidden), ctx=ctx)* .01\n",
        "Who = nd.random_normal(shape=(num_hidden,num_hidden), ctx=ctx)* .01\n",
        "\n",
        "########################\n",
        "#  Bias vector for hidden layer\n",
        "########################\n",
        "bg = nd.random_normal(shape=num_hidden, ctx=ctx) * .01\n",
        "bi = nd.random_normal(shape=num_hidden, ctx=ctx) * .01\n",
        "bf = nd.random_normal(shape=num_hidden, ctx=ctx) * .01\n",
        "bo = nd.random_normal(shape=num_hidden, ctx=ctx) * .01\n",
        "\n",
        "########################\n",
        "# Weights to the output nodes\n",
        "########################\n",
        "Why = nd.random_normal(shape=(num_hidden,num_outputs), ctx=ctx) * .01\n",
        "by = nd.random_normal(shape=num_outputs, ctx=ctx) * .01"
      ],
      "metadata": {
        "id": "kW5Ks-6bVVki"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Attach the gradient\n",
        "params = [Wxg, Wxi, Wxf, Wxo, Whg, Whi, Whf, Who, bg, bi, bf, bo, Why, by]\n",
        "\n",
        "for param in params:\n",
        "    param.attach_grad()"
      ],
      "metadata": {
        "id": "75TuFSo5VX7F"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax Activation\n",
        "def softmax(y_linear, temperature=1.0):\n",
        "    lin = (y_linear-nd.max(y_linear)) / temperature\n",
        "    exp = nd.exp(lin)\n",
        "    partition = nd.sum(exp, axis=0, exclude=True).reshape((-1,1))\n",
        "    return exp / partition"
      ],
      "metadata": {
        "id": "QnqhBfS-Vitw"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "def lstm_rnn(inputs, h, c, temperature=1.0):\n",
        "    outputs = []\n",
        "    for X in inputs:\n",
        "        g = nd.tanh(nd.dot(X, Wxg) + nd.dot(h, Whg) + bg)\n",
        "        i = nd.sigmoid(nd.dot(X, Wxi) + nd.dot(h, Whi) + bi)\n",
        "        f = nd.sigmoid(nd.dot(X, Wxf) + nd.dot(h, Whf) + bf)\n",
        "        o = nd.sigmoid(nd.dot(X, Wxo) + nd.dot(h, Who) + bo)\n",
        "        #######################\n",
        "        #\n",
        "        #######################\n",
        "        c = f * c + i * g\n",
        "        h = o * nd.tanh(c)\n",
        "        #######################\n",
        "        #\n",
        "        #######################\n",
        "        yhat_linear = nd.dot(h, Why) + by\n",
        "        yhat = softmax(yhat_linear, temperature=temperature)\n",
        "        outputs.append(yhat)\n",
        "    return (outputs, h, c)"
      ],
      "metadata": {
        "id": "u7idJt6RVkfL"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-entropy loss function\n",
        "def cross_entropy(yhat, y):\n",
        "    return - nd.mean(nd.sum(y * nd.log(yhat), axis=0, exclude=True))"
      ],
      "metadata": {
        "id": "izskR6aeVmOl"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Averaging the loss over the sequence\n",
        "def average_ce_loss(outputs, labels):\n",
        "    assert(len(outputs) == len(labels))\n",
        "    total_loss = 0.\n",
        "    for (output, label) in zip(outputs,labels):\n",
        "        total_loss = total_loss + cross_entropy(output, label)\n",
        "    return total_loss / len(outputs)"
      ],
      "metadata": {
        "id": "uzQksblPVpGv"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use SGD for optimizer\n",
        "def SGD(params, lr):\n",
        "    for param in params:\n",
        "        param[:] = param - lr * param.grad"
      ],
      "metadata": {
        "id": "rex_UMFzVqL1"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating sample text\n",
        "def sample(prefix, num_chars, temperature=1.0):\n",
        "    #####################################\n",
        "    # Initialize the string that we'll return to the supplied prefix\n",
        "    #####################################\n",
        "    string = prefix\n",
        "\n",
        "    #####################################\n",
        "    # Prepare the prefix as a sequence of one-hots for ingestion by RNN\n",
        "    #####################################\n",
        "    prefix_numerical = [character_dict[char] for char in prefix]\n",
        "    input_sequence = one_hots(prefix_numerical)\n",
        "\n",
        "    #####################################\n",
        "    # Set the initial state of the hidden representation ($h_0$) to the zero vector\n",
        "    #####################################\n",
        "    h = nd.zeros(shape=(1, num_hidden), ctx=ctx)\n",
        "    c = nd.zeros(shape=(1, num_hidden), ctx=ctx)\n",
        "\n",
        "    #####################################\n",
        "    # For num_chars iterations,\n",
        "    #     1) feed in the current input\n",
        "    #     2) sample next character from from output distribution\n",
        "    #     3) add sampled character to the decoded string\n",
        "    #     4) prepare the sampled character as a one_hot (to be the next input)\n",
        "    #####################################\n",
        "    for i in range(num_chars):\n",
        "        outputs, h, c = lstm_rnn(input_sequence, h, c, temperature=temperature)\n",
        "        choice = np.random.choice(vocab_size, p=outputs[-1][0].asnumpy())\n",
        "        string += character_list[choice]\n",
        "        input_sequence = one_hots([choice])\n",
        "    return string"
      ],
      "metadata": {
        "id": "c7kwduiFVrxg"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "epochs = 20\n",
        "moving_loss = 0.0\n",
        "\n",
        "learning_rate = 0.8\n",
        "\n",
        "# state = nd.zeros(shape=(batch_size, num_hidden), ctx=ctx)\n",
        "for e in range(epochs):\n",
        "    ############################\n",
        "    # Attenuate the learning rate by a factor of 2 every 100 epochs.\n",
        "    ############################\n",
        "    if ((e+1) % 100 == 0):\n",
        "        learning_rate = learning_rate / 2.0\n",
        "    h = nd.zeros(shape=(batch_size, num_hidden), ctx=ctx)\n",
        "    c = nd.zeros(shape=(batch_size, num_hidden), ctx=ctx)\n",
        "    for i in range(num_batches):\n",
        "        data_one_hot = train_data[i]\n",
        "        label_one_hot = train_label[i]\n",
        "        with autograd.record():\n",
        "            outputs, h, c = lstm_rnn(data_one_hot, h, c)\n",
        "            loss = average_ce_loss(outputs, label_one_hot)\n",
        "            loss.backward()\n",
        "        SGD(params, learning_rate)\n",
        "\n",
        "        ##########################\n",
        "        #  Keep a moving average of the losses\n",
        "        ##########################\n",
        "        if (i == 0) and (e == 0):\n",
        "            moving_loss = nd.mean(loss).asscalar()\n",
        "        else:\n",
        "            moving_loss = .99 * moving_loss + .01 * nd.mean(loss).asscalar()\n",
        "\n",
        "    print(\"Epoch %s. Loss: %s\" % (e, moving_loss))\n",
        "    print(sample(\"i knew that\", 8, temperature=.1))\n",
        "    print(sample(\"clarke smiled incredulously\", 8, temperature=.1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dDQqcW9VtXY",
        "outputId": "d57b688f-c2ef-4335-cb60-5b002dc97eff"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0. Loss: 1.608720358219283\n",
            "i knew that onder a\n",
            "clarke smiled incredulously and the\n",
            "Epoch 1. Loss: 1.5924568814288824\n",
            "i knew that the wou\n",
            "clarke smiled incredulously and the\n",
            "Epoch 2. Loss: 1.5806342124652752\n",
            "i knew that in the \n",
            "clarke smiled incredulously and the\n",
            "Epoch 3. Loss: 1.5700049130738278\n",
            "i knew that it one \n",
            "clarke smiled incredulously beather\n",
            "Epoch 4. Loss: 1.5615850412323897\n",
            "i knew that the for\n",
            "clarke smiled incredulously and the\n",
            "Epoch 5. Loss: 1.5533091109995045\n",
            "i knew that i car a\n",
            "clarke smiled incredulously and the\n",
            "Epoch 6. Loss: 1.5429469301381036\n",
            "i knew that he had \n",
            "clarke smiled incredulously and the\n",
            "Epoch 7. Loss: 1.5350769467093863\n",
            "i knew that the for\n",
            "clarke smiled incredulously and the\n",
            "Epoch 8. Loss: 1.528042326352674\n",
            "i knew that in the \n",
            "clarke smiled incredulously and the\n",
            "Epoch 9. Loss: 1.5207776483088362\n",
            "i knew that it to o\n",
            "clarke smiled incredulously and the\n",
            "Epoch 10. Loss: 1.5122248030601364\n",
            "i knew that our of \n",
            "clarke smiled incredulously and the\n",
            "Epoch 11. Loss: 1.5044846816257824\n",
            "i knew that it of t\n",
            "clarke smiled incredulously and ste\n",
            "Epoch 12. Loss: 1.4980662879463045\n",
            "i knew that the gro\n",
            "clarke smiled incredulously and the\n",
            "Epoch 13. Loss: 1.4906746907814314\n",
            "i knew that it was \n",
            "clarke smiled incredulously and the\n",
            "Epoch 14. Loss: 1.4829830437377647\n",
            "i knew that it to w\n",
            "clarke smiled incredulously and the\n",
            "Epoch 15. Loss: 1.4774921412393558\n",
            "i knew that it in t\n",
            "clarke smiled incredulously and ste\n",
            "Epoch 16. Loss: 1.473448706756158\n",
            "i knew that the you\n",
            "clarke smiled incredulously a clark\n",
            "Epoch 17. Loss: 1.4662362283040522\n",
            "i knew that it in t\n",
            "clarke smiled incredulously and ste\n",
            "Epoch 18. Loss: 1.4588031842499949\n",
            "i knew that it in t\n",
            "clarke smiled incredulously and ste\n",
            "Epoch 19. Loss: 1.4507241625285816\n",
            "i knew that it in t\n",
            "clarke smiled incredulously and ste\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Enter your name**"
      ],
      "metadata": {
        "id": "8VinizavAoTa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Name: Pakorn\n",
        "\n",
        "Surname: Sagulkoo\n",
        "\n",
        "ID: 6581030520"
      ],
      "metadata": {
        "id": "lbxk8EkpAg0T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. RNN Design Architectures.\n",
        "**Do not use AI generated words (Chatgpt or Gimini) please. Write it from your understanding.**\n",
        "\n",
        "Describe (1.) (5 points) concept of RNN and LSTM briefly. (2.) Download data from gutenberg (by import nltk) and choose some data in gutenberg.fileids() to clean. If the code do not have any errors in download, then I will give you 5 points. Next, if you can clean the data, then I will give you another 5 points. Next, if you use the cleaned data to fit RNN or LSTM model and do not have any errors, then I will give you another 10 points.    \n",
        "\n",
        "So, there are 10 points and plus 15 extra points.\n",
        "\n",
        "**Note that,** you can write in this file or you can write it in your word and allow you to type in **Thai**. You can sent **this file** or **.pdf** file to me."
      ],
      "metadata": {
        "id": "rACT4cUjj_Cw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For you answers,\n",
        "\n"
      ],
      "metadata": {
        "id": "HxJzl22EpITm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Simple RNN\n",
        "\n",
        "The concept of vanila or simple RNN is to forecast the next character of the recent input (character) based on the activation function between the recent hidden state ($\\begin{aligned}\\mathbf{h}_t\\end{aligned}$) and the weight of the hidden state and the output ($\\begin{aligned}\\mathbf{W}_\\textrm{hy}\\end{aligned}$) according to the equation below.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbf{y}_t &= \\textrm{softmax}(\\mathbf{W}_\\textrm{hy} \\mathbf{h}_t + \\mathbf{b}_\\textrm{y}),\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "where $\\begin{aligned}\\mathbf{y}_t\\end{aligned}$ is the recent output and $\\begin{aligned}\\mathbf{b}_y\\end{aligned}$ is a bias.The recent hidden state ($\\begin{aligned}\\mathbf{h}_t\\end{aligned}$) is calculated from the previous hidden state ($\\begin{aligned}\\mathbf{h}_\\textrm{t-1}\\end{aligned}$) as the equation\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbf{h}_t &= \\textrm{tanh}(\\mathbf{W}_\\textrm{hh} \\mathbf{h}_\\textrm{t-1} + \\mathbf{W}_\\textrm{xh} \\mathbf{x}_\\textrm{t} + \\mathbf{b}_\\textrm{h}),\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "where $\\mathbf{x}_\\textrm{t}, \\mathbf{W}_\\textrm{hh}, \\mathbf{W}_\\textrm{xh},$ and $\\mathbf{b}_\\textrm{h}$ are the recent input, weight for hidden-hidden state, weight for hidden-input, and bias for hidden state calculation, respectively. This assumption is based on the classical dynamic time concept that the hidden state has an influence to predict the data where by the recent hidden state is based on the activation function of the linear combination between the recent input and the previous hidden state."
      ],
      "metadata": {
        "id": "p86dlPD3kosy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. LSTM\n",
        "\n",
        "LSTM has a different concept from the simple RNN by the former has an idea of short- and long-term hidden state. For example, the LSTM has an assumption that combination of short- and long-term hidden state just like short- and long-term memmories in human can improve the character or word forecasting better than using only short-term hidden state (hidden state in the simple RNN). We describe more detail according to the equation.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbf{i}_t &= \\sigma(\\mathbf{W}_{\\textrm{xi}}\\mathbf{x}_t + \\mathbf{W}_{\\textrm{hi}}\\mathbf{h}_{t-1}  + \\mathbf{b}_\\textrm{i}),\\\\\n",
        "\\mathbf{f}_t &= \\sigma(\\mathbf{W}_{\\textrm{xf}}\\mathbf{x}_t + \\mathbf{W}_{\\textrm{hf}}\\mathbf{h}_{t-1}  + \\mathbf{b}_\\textrm{f}),\\\\\n",
        "\\mathbf{o}_t &= \\sigma(\\mathbf{W}_{\\textrm{xo}}\\mathbf{x}_t + \\mathbf{W}_{\\textrm{ho}}\\mathbf{h}_{t-1}  + \\mathbf{b}_\\textrm{o}),\\\\\n",
        "\\mathbf{g}_t &= \\textrm{tanh}(\\mathbf{W}_{\\textrm{xg}}\\mathbf{x}_t + \\mathbf{W}_{\\textrm{hg}}\\mathbf{h}_{t-1}  + \\mathbf{b}_\\textrm{g}),\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "where $\\mathbf{i}_\\textrm{t}, \\mathbf{f}_\\textrm{t}, \\mathbf{o}_\\textrm{t}, $ and $\\mathbf{g}_\\textrm{t}$ are the recent input, forget, output, and gate gate, respectively. The LSTM has the two main component such as the short- and long-term hidden state. The long-term hidden state ($\\mathbf{c}_\\textrm{t}$) permits for forget some values during the rescent state to maintain long membory value to improve the prediction performance, according this equation\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbf{c}_t = \\mathbf{f}_t \\odot \\mathbf{c}_{t-1} + \\mathbf{i}_t \\odot \\mathbf{g}_t,\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "where $\\odot$ is Hadamard (elementwise) product operator. To calculate the short-term hidden state, it is calculated based on this equation\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbf{h}_t = \\mathbf{o}_t \\odot \\textrm{tanh}(\\mathbf{c}_{t-1})\n",
        "\\end{aligned}.\n",
        "$$\n",
        "\n",
        "After that, the recent output is calculated like the simple RNN as the equation below\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbf{y}_t &= \\textrm{softmax}(\\mathbf{W}_\\textrm{hy} \\mathbf{h}_t + \\mathbf{b}_\\textrm{y}).\n",
        "\\end{aligned}\n",
        "$$"
      ],
      "metadata": {
        "id": "aBLuFubLG6Vd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Case study of RNN with the Gutenberg project"
      ],
      "metadata": {
        "id": "e7L1vGQ4i9hX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.1 Download the data of the Gutenberg project"
      ],
      "metadata": {
        "id": "EazWzgpksXms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mxnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyTsJboFlFPi",
        "outputId": "44f54847-e4bb-4c27-9163-8d4204afa441"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mxnet in /usr/local/lib/python3.10/dist-packages (1.9.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (1.23.1)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (2.31.0)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from mxnet) (0.8.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mxnet as mx\n",
        "from mxnet import nd, autograd\n",
        "import numpy as np\n",
        "mx.random.seed(1)\n",
        "ctx = mx.cpu()"
      ],
      "metadata": {
        "id": "veseIuHTlUo5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "files = nltk.corpus.gutenberg.fileids()\n",
        "print(files)"
      ],
      "metadata": {
        "id": "sq2KfLNcXzkz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cddb43fc-9562-4f29-b835-ac50618ef0bd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2 Exploring the data"
      ],
      "metadata": {
        "id": "o6mDOQCgshJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download some book and convert it to a list\n",
        "from nltk.corpus import gutenberg\n",
        "emma = gutenberg.words('austen-emma.txt')\n",
        "emma = list(emma)"
      ],
      "metadata": {
        "id": "l0a835cHkxjL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert a list into a string variable\n",
        "pg = ' '.join(emma)"
      ],
      "metadata": {
        "id": "BCmWP2_nqnPM"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(pg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VA6_qcgrgF1",
        "outputId": "69a3f27d-c5a4-4ec4-a690-3c78868f9e94"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "915041"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore the first 300 characters\n",
        "pg[:300]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "eJwJMXO2rhuN",
        "outputId": "456a7e53-a3ba-40d7-e6e2-320c069177cc"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[ Emma by Jane Austen 1816 ] VOLUME I CHAPTER I Emma Woodhouse , handsome , clever , and rich , with a comfortable home and happy disposition , seemed to unite some of the best blessings of existence ; and had lived nearly twenty - one years in the world with very little to distress or vex her . She'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore the last 300 characters\n",
        "pg[-300:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "D8XwKIg-rkQ9",
        "outputId": "ee6cea76-40bb-417f-b7e9-bd140c523c31"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'; a most pitiful business !-- Selina would stare when she heard of it .\"-- But , in spite of these deficiencies , the wishes , the hopes , the confidence , the predictions of the small band of true friends who witnessed the ceremony , were fully answered in the perfect happiness of the union . FINIS'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because when I look at the fist and the last 300 characters of the book and there is no contents not relevant to the main story, it is unneccessary to remove any characters."
      ],
      "metadata": {
        "id": "T642hx3NsnUW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.3 Data cleaning and pre-processing"
      ],
      "metadata": {
        "id": "MfDjvsQnsoXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all characters to lower cases\n",
        "pg = pg.lower()"
      ],
      "metadata": {
        "id": "kCPs3ScptK_Z"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove space before some special characters\n",
        "pg = pg.replace(' .', '.')\n",
        "pg = pg.replace(' ,', ',')\n",
        "pg = pg.replace(' ;', ';')\n",
        "pg = pg.replace(' :', ':')\n",
        "pg = pg.replace(' ?', '?')\n",
        "pg = pg.replace(' !', '!')\n",
        "pg = pg.replace(' - ', '-')\n",
        "pg = pg.replace(\" ' \", \"'\")"
      ],
      "metadata": {
        "id": "-bVnS0wwyOF1"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pg[1000:3500]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "5ZvAtW4RyY1W",
        "outputId": "f0329fd5-e4a1-4a1c-8b5f-8141531067ef"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"emper had hardly allowed her to impose any restraint; and the shadow of authority being now long passed away, they had been living together as friend and friend very mutually attached, and emma doing just what she liked; highly esteeming miss taylor's judgment, but directed chiefly by her own. the real evils, indeed, of emma's situation were the power of having rather too much her own way, and a disposition to think a little too well of herself; these were the disadvantages which threatened alloy to her many enjoyments. the danger, however, was at present so unperceived, that they did not by any means rank as misfortunes with her. sorrow came -- a gentle sorrow -- but not at all in the shape of any disagreeable consciousness.-- miss taylor married. it was miss taylor's loss which first brought grief. it was on the wedding-day of this beloved friend that emma first sat in mournful thought of any continuance. the wedding over, and the bride-people gone, her father and herself were left to dine together, with no prospect of a third to cheer a long evening. her father composed himself to sleep after dinner, as usual, and she had then only to sit and think of what she had lost. the event had every promise of happiness for her friend. mr. weston was a man of unexceptionable character, easy fortune, suitable age, and pleasant manners; and there was some satisfaction in considering with what self-denying, generous friendship she had always wished and promoted the match; but it was a black morning's work for her. the want of miss taylor would be felt every hour of every day. she recalled her past kindness -- the kindness, the affection of sixteen years -- how she had taught and how she had played with her from five years old -- how she had devoted all her powers to attach and amuse her in health -- and how nursed her through the various illnesses of childhood. a large debt of gratitude was owing here; but the intercourse of the last seven years, the equal footing and perfect unreserve which had soon followed isabella's marriage, on their being left to each other, was yet a dearer, tenderer recollection. she had been a friend and companion such as few possessed: intelligent, well-informed, useful, gentle, knowing all the ways of the family, interested in all its concerns, and peculiarly interested in herself, in every pleasure, every scheme of hers -- one to whom she could speak every thought as it arose, and who had such an affection for her as could never find fau\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Transform the character into vector\n",
        "character_list = list(set(pg))\n",
        "vocab_size = len(character_list)\n",
        "print(character_list)\n",
        "print(\"Length of vocab: %s\" % vocab_size)\n",
        "# Store them in dictionary\n",
        "character_dict = {}\n",
        "for e, char in enumerate(character_list):\n",
        "    character_dict[char] = e\n",
        "print(character_dict)\n",
        "pg_numerical = [character_dict[char] for char in pg]\n",
        "#  Check that the length is right\n",
        "print(len(pg))\n",
        "print(pg_numerical[:20])\n",
        "print(\"\".join([character_list[idx] for idx in pg_numerical[:39]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKw0O-_FuWPk",
        "outputId": "82cebe28-0151-4284-f518-215c87bb36d9"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['n', 'u', 'h', 'w', ']', '!', 'f', ',', 't', 'j', 'm', ';', 'q', '4', ')', '8', '(', '.', 'l', 'p', '\"', 'o', ':', 'x', '-', '3', '&', '6', '0', 'g', '7', 'r', 'v', '1', 'c', 'y', '[', \"'\", '?', '`', '_', 'z', 'k', 'a', ' ', 'i', 'd', 's', 'e', '2', 'b']\n",
            "Length of vocab: 51\n",
            "{'n': 0, 'u': 1, 'h': 2, 'w': 3, ']': 4, '!': 5, 'f': 6, ',': 7, 't': 8, 'j': 9, 'm': 10, ';': 11, 'q': 12, '4': 13, ')': 14, '8': 15, '(': 16, '.': 17, 'l': 18, 'p': 19, '\"': 20, 'o': 21, ':': 22, 'x': 23, '-': 24, '3': 25, '&': 26, '6': 27, '0': 28, 'g': 29, '7': 30, 'r': 31, 'v': 32, '1': 33, 'c': 34, 'y': 35, '[': 36, \"'\": 37, '?': 38, '`': 39, '_': 40, 'z': 41, 'k': 42, 'a': 43, ' ': 44, 'i': 45, 'd': 46, 's': 47, 'e': 48, '2': 49, 'b': 50}\n",
            "886804\n",
            "[36, 44, 48, 10, 10, 43, 44, 50, 35, 44, 9, 43, 0, 48, 44, 43, 1, 47, 8, 48]\n",
            "[ emma by jane austen 1816 ] volume i c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Use the one hot encoder\n",
        "def one_hots(numerical_list, vocab_size=vocab_size):\n",
        "    result = nd.zeros((len(numerical_list), vocab_size), ctx=ctx)\n",
        "    for i, idx in enumerate(numerical_list):\n",
        "        result[i, idx] = 1.0\n",
        "    return result\n",
        "def textify(embedding):\n",
        "    result = \"\"\n",
        "    indices = nd.argmax(embedding, axis=1).asnumpy()\n",
        "    for idx in indices:\n",
        "        result += character_list[int(idx)]\n",
        "    return result\n",
        "\n",
        "textify(one_hots(pg_numerical[0:40]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ssoN7UFJ1u7Y",
        "outputId": "429b79f9-2149-447b-86b2-f4fefb9af419"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[ emma by jane austen 1816 ] volume i ch'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the remaining division of the total characters and seq_length\n",
        "len(pg)//100, len(pg)%100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqgMe0zh2M10",
        "outputId": "2cbabf26-30f5-438c-e812-b4ab6d4e507f"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8868, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100\n",
        "# -4 here so we have enough characters for labels later\n",
        "num_samples = (len(pg_numerical) - 4) // seq_length\n",
        "dataset = one_hots(pg_numerical[:seq_length*num_samples]).reshape((num_samples, seq_length, vocab_size))\n",
        "textify(dataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "SSaPRHMm1873",
        "outputId": "55e0b84f-312a-4443-dc2c-a34e877906a4"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[ emma by jane austen 1816 ] volume i chapter i emma woodhouse, handsome, clever, and rich, with a c'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch preparation\n",
        "batch_size = 32\n",
        "print('# of sequences in dataset: ', len(dataset))\n",
        "num_batches = len(dataset) // batch_size\n",
        "print('# of batches: ', num_batches)\n",
        "train_data = dataset[:num_batches*batch_size].reshape((batch_size, num_batches, seq_length, vocab_size))\n",
        "# swap batch_size and seq_length axis to make later access easier\n",
        "train_data = nd.swapaxes(train_data, 0, 1)\n",
        "train_data = nd.swapaxes(train_data, 1, 2)\n",
        "print('Shape of data set: ', train_data.shape)\n",
        "\n",
        "for i in range(3):\n",
        "    print(\"***Batch %s:***\\n %s \\n %s \\n\\n\" % (i, textify(train_data[i, :, 0]), textify(train_data[i, :, 1])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7UKAk4-36gF",
        "outputId": "8c9682a6-6e85-4420-cb64-0a86e2f0d35a"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of sequences in dataset:  8868\n",
            "# of batches:  277\n",
            "Shape of data set:  (277, 100, 32, 51)\n",
            "***Batch 0:***\n",
            " [ emma by jane austen 1816 ] volume i chapter i emma woodhouse, handsome, clever, and rich, with a c \n",
            " it was all gone. there was a strange rumour in highbury of all the little perrys being seen with a s \n",
            "\n",
            "\n",
            "***Batch 1:***\n",
            " omfortable home and happy disposition, seemed to unite some of the best blessings of existence; and  \n",
            " lice of mrs. weston's wedding-cake in their hands: but mr. woodhouse would never believe it. chapter \n",
            "\n",
            "\n",
            "***Batch 2:***\n",
            " had lived nearly twenty-one years in the world with very little to distress or vex her. she was the  \n",
            "  iii mr. woodhouse was fond of society in his own way. he liked very much to have his friends come a \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(labels), batch_size*num_batches*seq_length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWFoYsC26RD9",
        "outputId": "0a1d31b2-23aa-4b83-b08c-84c45390cfb2"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(886400, 886400)"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 400 is removed to match the shape of the train_label\n",
        "labels = one_hots(pg_numerical[1:seq_length*num_samples+1-400])\n",
        "train_label = labels.reshape((batch_size, num_batches, seq_length, vocab_size))\n",
        "train_label = nd.swapaxes(train_label, 0, 1)\n",
        "train_label = nd.swapaxes(train_label, 1, 2)\n",
        "print(train_label.shape)\n",
        "\n",
        "print(textify(train_data[10, :, 3]))\n",
        "print(textify(train_label[10, :, 3]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaPT8MEd4KHY",
        "outputId": "89fa257f-a3db-4d2b-a565-cdf320e66a54"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(277, 100, 32, 51)\n",
            "he got back to mrs. goddard's, that mr. martin had been there an hour before, and finding she was no\n",
            "e got back to mrs. goddard's, that mr. martin had been there an hour before, and finding she was not\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.4 Model buiding"
      ],
      "metadata": {
        "id": "BwP-8VgV4STX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Simple RNN"
      ],
      "metadata": {
        "id": "BuFeVklc4YA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the parameters\n",
        "num_inputs, num_hidden, num_outputs = vocab_size, 256, vocab_size\n",
        "#  Weights connecting the inputs to the hidden layer\n",
        "Wxh = nd.random_normal(shape=(num_inputs, num_hidden), ctx=ctx) * 0.5\n",
        "#  Recurrent weights connecting the hidden layer across time steps\n",
        "Whh = nd.random_normal(shape=(num_hidden, num_hidden), ctx=ctx)  * 0.5\n",
        "#  Bias vector for hidden layer\n",
        "bh = nd.random_normal(shape=num_hidden, ctx=ctx)  * 0.5\n",
        "# Weights to the output nodes\n",
        "Why = nd.random_normal(shape=(num_hidden, num_outputs), ctx=ctx) * 0.5\n",
        "by = nd.random_normal(shape=num_outputs, ctx=ctx) * 0.5\n",
        "\n",
        "params = [Wxh, Whh, bh, Why, by]\n",
        "\n",
        "for param in params:\n",
        "    param.attach_grad()\n",
        "\n",
        "def softmax(y_linear, temperature=1.0):\n",
        "    lin = (y_linear-nd.max(y_linear, axis=1).reshape((-1,1))) / temperature # shift each row of y_linear by its max\n",
        "    exp = nd.exp(lin)\n",
        "    partition =nd.sum(exp, axis=1).reshape((-1,1))\n",
        "    return exp / partition\n",
        "\n",
        "# With a temperature of 1, get back some set of probabilities\n",
        "softmax(nd.array([[1, -1], [-1, 1]]), temperature=1.0)\n",
        "\n",
        "# With a high temperature, get more entropic (*noisier*) probabilities\n",
        "softmax(nd.array([[1,-1],[-1,1]]), temperature=1000.0)\n",
        "\n",
        "# With low temperatures to produce sharp probabilities\n",
        "softmax(nd.array([[10,-10],[-10,10]]), temperature=.1)\n",
        "\n",
        "def simple_rnn(inputs, state, temperature=1.0):\n",
        "    outputs = []\n",
        "    h = state\n",
        "    for X in inputs:\n",
        "        h_linear = nd.dot(X, Wxh) + nd.dot(h, Whh) + bh\n",
        "        h = nd.tanh(h_linear)\n",
        "        yhat_linear = nd.dot(h, Why) + by\n",
        "        yhat = softmax(yhat_linear, temperature=temperature)\n",
        "        outputs.append(yhat)\n",
        "    return (outputs, h)"
      ],
      "metadata": {
        "id": "3ppeJTpT4LIQ"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy(yhat, y):\n",
        "    return - nd.mean(nd.sum(y * nd.log(yhat), axis=0, exclude=True))\n",
        "cross_entropy(nd.array([[.2,.5,.3], [.2,.5,.3]]), nd.array([[1.,0,0], [0, 1.,0]]))\n",
        "\n",
        "# Averaging the loss over the sequence\n",
        "def average_ce_loss(outputs, labels):\n",
        "    assert(len(outputs) == len(labels))\n",
        "    total_loss = 0.\n",
        "    for (output, label) in zip(outputs,labels):\n",
        "        total_loss = total_loss + cross_entropy(output, label)\n",
        "    return total_loss / len(outputs)\n",
        "\n",
        "# Optimizer\n",
        "def SGD(params, lr):\n",
        "    for param in params:\n",
        "        param[:] = param - lr * param.grad\n",
        "\n",
        "def sample(prefix, num_chars, temperature=1.0):\n",
        "    # Initialize the string that we'll return to the supplied prefix\n",
        "    string = prefix\n",
        "    # Prepare the prefix as a sequence of one-hots for ingestion by RNN\n",
        "    prefix_numerical = [character_dict[char] for char in prefix]\n",
        "    input_sequence = one_hots(prefix_numerical)\n",
        "    # Set the initial state of the hidden representation ($h_0$) to the zero vector\n",
        "    sample_state = nd.zeros(shape=(1, num_hidden), ctx=ctx)\n",
        "    for i in range(num_chars):\n",
        "        outputs, sample_state = simple_rnn(input_sequence, sample_state, temperature=temperature)\n",
        "        choice = np.random.choice(vocab_size, p=outputs[-1][0].asnumpy())\n",
        "        string += character_list[choice]\n",
        "        input_sequence = one_hots([choice])\n",
        "    return string"
      ],
      "metadata": {
        "id": "blzQzH6E4ir0"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs, moving_loss, learning_rate = 5, 1, 0.05\n",
        "for e in range(epochs):\n",
        "    # Attenuate the learning rate by a factor of 2 every 10 epochs.\n",
        "    if ((e+1) % 10 == 0):\n",
        "        learning_rate = learning_rate / 2.0\n",
        "    state = nd.zeros(shape=(batch_size, num_hidden), ctx=ctx)\n",
        "    for i in range(num_batches):\n",
        "        data_one_hot = train_data[i]\n",
        "        label_one_hot = train_label[i]\n",
        "        with autograd.record():\n",
        "            outputs, state = simple_rnn(data_one_hot, state)\n",
        "            loss = average_ce_loss(outputs, label_one_hot)\n",
        "            loss.backward()\n",
        "        SGD(params, learning_rate)\n",
        "        #  Keep a moving average of the losses\n",
        "        if (i == 0) and (e == 0):\n",
        "            moving_loss = np.mean(loss.asnumpy()[0])\n",
        "        else:\n",
        "            moving_loss = .99 * moving_loss + .01 * np.mean(loss.asnumpy()[0])\n",
        "    print(\"Epoch %s. Loss: %s\" % (e, moving_loss))\n",
        "    print(sample(\"the world with\", 1024, temperature=.1))\n",
        "    print(sample(\"seemed to unite\", 1024, temperature=.1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PUKIdmi4mH4",
        "outputId": "6dcf8afc-12f2-480f-c94c-36c00f0d836c"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0. Loss: 2.927207711279123\n",
            "the world with me t b a he a t a t ae a b to aech a ae a aeao a a to wo t o \"oo ae aoe toe s  aer a th t toer h mo t d a t to ae t to te to a ter wa we tot t te hel s to t we touea i te t to m w toe t t ao ao he her aer h a t he t t a a t th  a a t t a w ao t to ter ao aer t m w w aea h t m to t t t ahea we te t s he ber a aer toe he m b he he w he woe toe toe te he s t a ao w w w te ae  t th wo w t t  to t t t w w be t h he a t te t the jo ao ao a ae toeb s toe boe a te t a h t ae to t der t he s wo s bg a mo t aera t a t wea he  an the t i aer a te to a to h toe t a to ao he he a h a tra won toa a a we tot t (he r m b ae aer an anerer he t hea t h s ter he t t h a to te w t ah se ao a ao nne s to ae t s he h ae a t ter a s t tero th an a a t ae t t ae te t t t te ae phe i he t te hete ho a t te to w s io t t tero a a w t a t ao s b ao a he ao t t woe bes mo te a t a w tet a t t h woer b to t s s to t w her te wo t a t ao a toe toe to a ae t me t a t w weto b t to te ao c he a toe me  ao aet to aer ce w to ae to to to he \n",
            "seemed to unite an wo t aet a a t te t t to t t hie aere b a a w weto b a to te she h t t te m he i a te h a a tnerer he t hea to ao w ao a t aea t a tx sh s a mhe t t t f a t he ae t to whe t t ae r he c d a h a te ho a t s mo t t her ao he ter to w w t to se a b a t t s g t he th oe t we to t t t a t t her t t t tooe hlt aooe a a a wo te ao toe t t qer we w tn s t ao s w a te m wer a s fot ser ae h whe w to touo to a ae a t he te ae t to w a h a aer t he t ae a t ao t t w te s hert i ao her t ta pe w t t to w ao he ae her a t t we t mh to w a h w w her we wo t a t ae t t a ao to toe t w te a tne a to te ie ao f te te the hen he w a s t th aoa a a ao ao be w t ho te a ae ao he aer w so ae \" s st ta a a ao t mo th t her b w co t te ao be t t t t ao t to s m to p t w s to s mo a a tie he s ber h o t te t we we toe ae a t te t b b a t ao a te t he a t s to to moe aerere ae mo \" t h w toe t t a aer t he a te b m terere a ae to to t ae a t t t he a w t h te t t ao t ao te ter t t wo t t w th to to a wh t be t t tes m ae te her \n",
            "Epoch 1. Loss: 2.9060675941831873\n",
            "the world with ae a t a he a a te t ae tn he t w be toe aer  w aoe t te te t s t ao to a t an s io ber be toe to t ah a aa a w w t a te wo to ae a t to t tt her t th a hoe t th her ae to to he h be t t t h ao to a tn re aea o  ao t a to to a ano ae t te a he s ae ae te t t s s se t aoer a ae te h ae b t t t  aoe t te tner s ththe her a t t he a t t t a hem d a a aa he  he a t to t whenoe he ter ao a to a too \"o tn b to te t ao a to b ter te ahe t t h toe t t a ae toto m t to a t w t a w we  a \" he ae boe to te a s to t ao t t ih t h a tererer tn s t to t t t s io t te \"h t je t a h ao  ao t t te a s to t ao t t to a h to toe a a aer a wer t a w s wo t o a ao he t to a to a te wo he her o a t wa te x t r s ( h t m t t t t to t t b w ae i h w ta to a ae y ao t ses aa a a t po ao to he h be t t q her tner t a aou a a ao a he t ` t he a ae te to m he a a a w he t ae r t a a to a s t to t t tere t h t to t we t a a w wet a t t h wne ter tot t tt be a t tea t t her h tol ae we wo a h t t to a ae tn t i w te s a h _o s w t ae t i\n",
            "seemed to unite a whe ao aerek ko i a to a coer  ao t a t h t t a t he aere h t _ b to t w ter s he te te w to a a to aoe h ao tea tn a te s to ao whe w h a a we t o t toe a ao ao ae tont bno w to t ao h t t a ao ae t wo h te h t a aaexoer hae me w b t te t w to a t ae t w t we s to toe t t ae ao ber t t t to toe a a t he a j t to te t h te he te aer th p t ie a h toe s to a t ae t b t ahe a t t ao ao a t t w b w ae t h te ae he a o a we me w te toue a oe a a h  b t woo te her t he a t ho w woe ae h s t hel a ae he a n t t t t t hoe b h t ae b t th to a ae toa a her t a th a t to t we t t th a a t t to w tos te ao sh s m t ao oo to hec t th ie w he a a he toen to t t t to t te b to mo to a  ao au t a as aea t the ae t w too t s w t t a t woe ao he a t te aot tea ae ae to a t t a aer he h te t h te he te aer to h ae a ao t t ao tn a tot tol a ae toer s toe ber iero t h he wo a h t t to a ae t t fe raer w ao w t ao woe he aoe w aon t he t t ao tou a b w ter tot t ti bes t t t w w ae  fot a th a a h to wue t te te a t wo s w t\n",
            "Epoch 2. Loss: 2.9008508041150263\n",
            "the world with a wo ae t a t he te he a toe w to t s s to w w her we wo t s t w t a a s m t he to a to a a t ao  h t toe t t t he a t an ao te te t  ah t a a a a her te ao te t t t a t b be a te aed   a t t t he w a a a a b a t t he to h t he b te aero aet _ j t b a h t he ae t a heah a toeao t f t ao a b t me t terer to  te  hheter th t te t tes m ae tem we a w t ae fa ae t ti a ao t to t t were t we  ao ah b t w a a t ae toe te ae g wo ah toto t h t te o a t ae ther ao s a a t a me toe te he te aer a t ao m ae ao t a rer he a ant ae w to wo t he a t her ho  b w a t h  h te a t s aof t te wo t he t t a a ao we t ae aerae moe te w tno t toe t t to a t a s _ i oer he t t t he h to he a te t b t w a t w he te a t a r a a to ao to he a a s t a he a t a to w to t a ae ae he a h t t t ae t a too a t to t ae a t ae t to t t t t w a w a te t t t her he th t t t t hep a t t h aere ter t a s h mo te t h ae ao b to t a too t t tou a t ao w a t t te ae ter s t a hen t t he  a t a at her te to h ao to a ane f a t o ao t t t s.e au b r\n",
            "seemed to unite a woot te j to a t ae he ae a a to m t w t oe a a wo ser bo t the ae te me he f t s te t a t a toey th t t he toe w m ae t to t a r t a be a tee w t to ao  te s wo ao f to t t tere t h t he s a ( a t to te st to a tnerer he t hea ao t t w a her t t ax ao a t we h t ti aoe t i we a h to t t ae t he a he aooe a t t te ao toe b to m s he to te te ao to toe t t ao a a a a we an t t ha wot s sea c t h t w her he th b th tot t to h he a ao a w m ho i t aes th we toe ae t ao t ae a heroe to he h be t t t h ae toe te he ta ao t tt aea t toe t a t b be r t s t t a to a a te he b s w bot th to w w t ao ae ae s  he t t a w wh mnn to t aer a t h w te th t h t a w toe t t a ae te w t ae aot s wo aot ao a a t a c s terer t t ae t b b a t ao a t tie ao t her ter w toi ho aerer t a a s t t toe w tht we ae ` t aer t b s t tero tot t t tes m ae te t t a s m s tot ao te aer t h t  h s t t t ( a t t t to a a b t t we a t te ae a t ter ae aoe s d t s ph we t t ah her h a t to s s a t wet a t t h woe a tio t i m to a toto to m a \n",
            "Epoch 3. Loss: 2.8972989410504257\n",
            "the world with t t a s to a a t s st t t hex ao a t t t a a ao  he wo to to toe a a a te  we t he  hhe he t he ao aev ar to ao aeaoe t toe t a t toe r a a t to te t t ao t h h wo a t w a her t to w to ao t ae s t t aoe c ` aea t ter w toi ho aer ao  ao t o t to a aer t t a a te a a v a t b a to a wo t a her ae w t oe t a to a ch s the a w te  s me aoer t (oe c to t to ao to a tno ael tes t a a t a t s he w oer t to b to to w wo t a ao m te m he w a t h a t t t ao s a a ae ao he toer to t t s to a te t s to to tha he ao ae to aer h t h te he s t w ao ae w te ta aoe t t to ao a a t a t bo t a an t a ae c a he w wo t t hoe w he ter t a t c to he t t w w wo ao a thi a toe te ae t ao toe b mo te s s _ to w t ao she a ae noer ter he f h he to he h be t t t t aoe ae a to t b toe ae ae an te ae ae to ae a t wo t he he her her a t t t a b h t t t t ao aoe yoe  a t be a aee w he he t a to a t a t hhe t t we t t a w wet a t t h toer b to t s h toe t t a toe h toe t t te a ao a beb a ti he ao noe h s he m ti s to te wh he ao th m tero\n",
            "seemed to unite a wooe a a t to  he w ae t h t t b w ae t h  aem hel a ao w a s i toht tt t a a t ao to ae ao he h be a tz t t to a b ae b he a ao s to aer t aea a to t mer ao ae the t h  oo t t w t w t w t w t w t w t t t t t w t w t h her ter ae  fot a wo t t s d he aoe m te m a t to ae a t h  b t wh ao t h t he tea w a w to t w t w t t ae t ae t he c a to tea t ao ae te te m he te w y t tohe he to ao to a t t th t t t co a a ao  he w iere a t t be h th r ae  a s t i h to t ao t t a ah w te m a ae a th tot t to h he a to a w m tht w t ti a ao to toe a w b a t ao to a an te b te t a ae aoer aes aon t t s t th w t s a t he a to s al roo t tn he to t he a we t t t p t a w tet a t t h tne ter tht a tohe \"e h a h wno te bo a t th ho ae b a b b t a ae toer aes aon h st a t t ah to a t bo t t tere t h t to t her ter ae a t tot jon t s wo t t a t h a t a to te he w te toe te he te ae s io tex a he ao a t he c t t an t b ao wo b h th he t t a te t wet a t t h woe a ther toe t t t aer a t h w te s w t tes m ae t t ao s h to t se ae\n",
            "Epoch 4. Loss: 2.894275236389925\n",
            "the world with a wo ae te a to a tne b t we se t wet t te he b iea ao t a t h be t t t h ao to a tne he t t \" t i t to m a t b te  a ao to a t a t he he  to t wo w po to t a t ao t tt t h ao s ie aoel her w t ae  f t a t tes m a t w he ao b t ao to t t to qer a te h a he p a t we ae  he s to te t a ae a o a ber a to a a a the ae t a s t t wea ao to t wo p aoo m h to t w t toe to a t te ao te a t t a the he t s t t ah s to ao t ta a w t t h a a t t t s m w a a ao th aa a he a to ao he w th t sh ao wao bo t te t a t w wet a t t h toe a t a too t t tere ter b t a s se a t t i aou a a ao a s t he a te s th c te t w ae a he to t t ae t a a s ter te t to t t tere t h t he t tex he h ae t  b te t m ao a he aero m t \" wo a t a th a a he f t ao ae ae t ao a woeaht s t a he t m b s aer b he to he ho  aof ae t he j h t ah a b a mo t he w t a a a b a t ae t w t t to t t tevo t to a te t ao t w her hfto ta he a ao th ao t a to t he a te a t ta a w t t w ae to t t ae a t  aea h a ter a pe t a a he w \"e w t t a te ao he t to wo t wo t t \n",
            "seemed to unite a woot t s to t t aex t a te ao  tot ae a h t t a te  fnt ao t t b w ae t h te he ta ao t t  s t ho w woe ae h s t hel he t s wo  a t he ao t h t a (oe ao a co a t her t t t too a a t ao t t a t t te t a too te w wo he aot to he ae co w t a s w w a t ao to w ier te a te t s to he t an t aer ce to t to t i a h a a a ah t he t aer j aoe to t we s aero a b bo j toe ao ae t h te he s a a ao w t t a b a her ae te a t ao ae t th aag th a he r ahe b ae a he s he a ters t t ae to to to be th t t ao h her te wo t a t ao a tht t t her w ao a to t m te tau b b a to whe t hero t t ha woe ao wo t we t t a w aet a t t hem t ao h a h to a t t a he h t f b t th a t ho b w t w te t te t a a w ao he tat ao t t boen s t me t we t a a w wet a t c t te a to ak the b ao w a a hea a aoe h ao to a tna aoer t woec b t whe :n t d a a t b w b bo j toe ao ae t h te aer t he a t to t her he t to to  ae he h te to t he b t t to to h t he a h w t t t ao h a m we te t t w a to he we  aer h t mo t wo ioe b t ho w m t t w ao th t ao he w he \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### LSTM"
      ],
      "metadata": {
        "id": "8FuRe-q74b-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize parameters\n",
        "num_inputs = vocab_size\n",
        "num_hidden = 256\n",
        "num_outputs = vocab_size\n",
        "\n",
        "########################\n",
        "#  Weights connecting the inputs to the hidden layer\n",
        "########################\n",
        "Wxg = nd.random_normal(shape=(num_inputs,num_hidden), ctx=ctx) * .01\n",
        "Wxi = nd.random_normal(shape=(num_inputs,num_hidden), ctx=ctx) * .01\n",
        "Wxf = nd.random_normal(shape=(num_inputs,num_hidden), ctx=ctx) * .01\n",
        "Wxo = nd.random_normal(shape=(num_inputs,num_hidden), ctx=ctx) * .01\n",
        "\n",
        "########################\n",
        "#  Recurrent weights connecting the hidden layer across time steps\n",
        "########################\n",
        "Whg = nd.random_normal(shape=(num_hidden,num_hidden), ctx=ctx)* .01\n",
        "Whi = nd.random_normal(shape=(num_hidden,num_hidden), ctx=ctx)* .01\n",
        "Whf = nd.random_normal(shape=(num_hidden,num_hidden), ctx=ctx)* .01\n",
        "Who = nd.random_normal(shape=(num_hidden,num_hidden), ctx=ctx)* .01\n",
        "\n",
        "########################\n",
        "#  Bias vector for hidden layer\n",
        "########################\n",
        "bg = nd.random_normal(shape=num_hidden, ctx=ctx) * .01\n",
        "bi = nd.random_normal(shape=num_hidden, ctx=ctx) * .01\n",
        "bf = nd.random_normal(shape=num_hidden, ctx=ctx) * .01\n",
        "bo = nd.random_normal(shape=num_hidden, ctx=ctx) * .01\n",
        "\n",
        "########################\n",
        "# Weights to the output nodes\n",
        "########################\n",
        "Why = nd.random_normal(shape=(num_hidden,num_outputs), ctx=ctx) * .01\n",
        "by = nd.random_normal(shape=num_outputs, ctx=ctx) * .01"
      ],
      "metadata": {
        "id": "i2qXlblR4sw2"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Attach the gradient\n",
        "params = [Wxg, Wxi, Wxf, Wxo, Whg, Whi, Whf, Who, bg, bi, bf, bo, Why, by]\n",
        "\n",
        "for param in params:\n",
        "    param.attach_grad()"
      ],
      "metadata": {
        "id": "_gKrs1194xZu"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax Activation\n",
        "def softmax(y_linear, temperature=1.0):\n",
        "    lin = (y_linear-nd.max(y_linear)) / temperature\n",
        "    exp = nd.exp(lin)\n",
        "    partition = nd.sum(exp, axis=0, exclude=True).reshape((-1,1))\n",
        "    return exp / partition"
      ],
      "metadata": {
        "id": "6oWQksIr400b"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "def lstm_rnn(inputs, h, c, temperature=1.0):\n",
        "    outputs = []\n",
        "    for X in inputs:\n",
        "        g = nd.tanh(nd.dot(X, Wxg) + nd.dot(h, Whg) + bg)\n",
        "        i = nd.sigmoid(nd.dot(X, Wxi) + nd.dot(h, Whi) + bi)\n",
        "        f = nd.sigmoid(nd.dot(X, Wxf) + nd.dot(h, Whf) + bf)\n",
        "        o = nd.sigmoid(nd.dot(X, Wxo) + nd.dot(h, Who) + bo)\n",
        "        #######################\n",
        "        #\n",
        "        #######################\n",
        "        c = f * c + i * g\n",
        "        h = o * nd.tanh(c)\n",
        "        #######################\n",
        "        #\n",
        "        #######################\n",
        "        yhat_linear = nd.dot(h, Why) + by\n",
        "        yhat = softmax(yhat_linear, temperature=temperature)\n",
        "        outputs.append(yhat)\n",
        "    return (outputs, h, c)"
      ],
      "metadata": {
        "id": "tPkkXSML41qm"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-entropy loss function\n",
        "def cross_entropy(yhat, y):\n",
        "    return - nd.mean(nd.sum(y * nd.log(yhat), axis=0, exclude=True))"
      ],
      "metadata": {
        "id": "1_55rTpq43qD"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Averaging the loss over the sequence\n",
        "def average_ce_loss(outputs, labels):\n",
        "    assert(len(outputs) == len(labels))\n",
        "    total_loss = 0.\n",
        "    for (output, label) in zip(outputs,labels):\n",
        "        total_loss = total_loss + cross_entropy(output, label)\n",
        "    return total_loss / len(outputs)"
      ],
      "metadata": {
        "id": "MGLhqkkX45bV"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use SGD for optimizer\n",
        "def SGD(params, lr):\n",
        "    for param in params:\n",
        "        param[:] = param - lr * param.grad"
      ],
      "metadata": {
        "id": "DDv9kgKT48Uj"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating sample text\n",
        "def sample(prefix, num_chars, temperature=1.0):\n",
        "    #####################################\n",
        "    # Initialize the string that we'll return to the supplied prefix\n",
        "    #####################################\n",
        "    string = prefix\n",
        "\n",
        "    #####################################\n",
        "    # Prepare the prefix as a sequence of one-hots for ingestion by RNN\n",
        "    #####################################\n",
        "    prefix_numerical = [character_dict[char] for char in prefix]\n",
        "    input_sequence = one_hots(prefix_numerical)\n",
        "\n",
        "    #####################################\n",
        "    # Set the initial state of the hidden representation ($h_0$) to the zero vector\n",
        "    #####################################\n",
        "    h = nd.zeros(shape=(1, num_hidden), ctx=ctx)\n",
        "    c = nd.zeros(shape=(1, num_hidden), ctx=ctx)\n",
        "\n",
        "    #####################################\n",
        "    # For num_chars iterations,\n",
        "    #     1) feed in the current input\n",
        "    #     2) sample next character from from output distribution\n",
        "    #     3) add sampled character to the decoded string\n",
        "    #     4) prepare the sampled character as a one_hot (to be the next input)\n",
        "    #####################################\n",
        "    for i in range(num_chars):\n",
        "        outputs, h, c = lstm_rnn(input_sequence, h, c, temperature=temperature)\n",
        "        choice = np.random.choice(vocab_size, p=outputs[-1][0].asnumpy())\n",
        "        string += character_list[choice]\n",
        "        input_sequence = one_hots([choice])\n",
        "    return string"
      ],
      "metadata": {
        "id": "Cwk2H6F_4-2f"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "epochs = 10\n",
        "moving_loss = 0.0\n",
        "\n",
        "learning_rate = 0.02\n",
        "\n",
        "# state = nd.zeros(shape=(batch_size, num_hidden), ctx=ctx)\n",
        "for e in range(epochs):\n",
        "    ############################\n",
        "    # Attenuate the learning rate by a factor of 2 every 100 epochs.\n",
        "    ############################\n",
        "    if ((e+1) % 100 == 0):\n",
        "        learning_rate = learning_rate / 2.0\n",
        "    h = nd.zeros(shape=(batch_size, num_hidden), ctx=ctx)\n",
        "    c = nd.zeros(shape=(batch_size, num_hidden), ctx=ctx)\n",
        "    for i in range(num_batches):\n",
        "        data_one_hot = train_data[i]\n",
        "        label_one_hot = train_label[i]\n",
        "        with autograd.record():\n",
        "            outputs, h, c = lstm_rnn(data_one_hot, h, c)\n",
        "            loss = average_ce_loss(outputs, label_one_hot)\n",
        "            loss.backward()\n",
        "        SGD(params, learning_rate)\n",
        "\n",
        "        ##########################\n",
        "        #  Keep a moving average of the losses\n",
        "        ##########################\n",
        "        if (i == 0) and (e == 0):\n",
        "            moving_loss = nd.mean(loss).asscalar()\n",
        "        else:\n",
        "            moving_loss = .99 * moving_loss + .01 * nd.mean(loss).asscalar()\n",
        "\n",
        "    print(\"Epoch %s. Loss: %s\" % (e, moving_loss))\n",
        "    print(sample(\"the world with\", 1024, temperature=.1))\n",
        "    print(sample(\"seemed to unite\", 1024, temperature=.1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpnnBEW-5ABX",
        "outputId": "fa098e87-1504-4827-f507-199b1e2aef11"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0. Loss: 1.4877082173591265\n",
            "the world with his being to be any the present of the present of the present of the sort of the present of the present of the sort of the party of the sort of the sort of the say of the present of the present of the sort of the present of the sort of the rest to have been a must be as the sort of the sure of a more to see the sort of the present of the present of the present of the sort of his being to a some of the sort of the sort of a miss better the proper of the present of the sure of the sort of the sort of the sort of the present of the sort of the present of the sort of the sort of her father and the present of the sure of the present of the sort of the sort of the sort of his being to the sort of the present of the sort of the sort of the sort of the sort of the sort of the present of the partion of the sort of the sort of the present of the sort of the sort of the sort of the sort of the sort of the sort of the sort of the sort of the sort of the sort of the sort of the sort of her father and so much and she was \n",
            "seemed to unitely to me to me to the more that i have not dear the such a sort of the sort of the sort of the sure of the partion of the sure of the sort of his being to be all the present of the present of the sort of the sort of the sort of the sort of the sort of the present of her father and she had not be all the present of the such a sort of the sort of the present of the sort of her fairfax of the sort of the sort of the sort of his being to be a more to be a more to any the sort of his such and the conserration of the say and she had not be all the present of the sort of a more to mane of the sort of the sure of the present of the sort of the sort of the sort of his being to be a more the could be as the present of the present of the sort of the sort of the sort of the sort of the sort of the present of the present of the present of the sort of the present of the present of the sure of the proper of the sire of her friends of the sort of the partion of the sort of the sort of the present of the present of the sort o\n",
            "Epoch 1. Loss: 1.4764980413083437\n",
            "the world with a sort of the sort of the sort of the sort of the sure of the say of her father and so being to be a more to be a more to be a more the sort of the sort of the sort of the sort of her of the sort of the sort of the sort of the sort of her friends of the sort of the present of the sure of the say of the sort of the sort of the sort of the sort of the sort of the sort of the say of the consertion of her friends the such a sort of her of the sort of the sort of the sort of the present of the contring to her forthing and the conserration of her father and the consertion of the sort of the sort of the say and the consertion of the sure of the sort of the sort of the sort of the sort of the sort of the sort of the sort of the sort of the say of her father and she was not the consertion of her fairfax of the say of the say of the sort of the say of the sort of the sort of the sort of the sort of her father and so being to her at the sort of the sort of the sort of the sort of the sort of the sort of the sort of the\n",
            "seemed to unite the say and she was not be all the present of the sort of her father and the sort of the sort of the sure of the sort of the sort of the sort of the present of the say of her father and she was to be a more to mane the such a sort of the sort of the sort of his father and she was a more to be a more to have been a more to be a more the say of the sort of the sort of the say of the sort of the present of the sort of the sort of the say of the sort of the sort of a some of the sort of her at the sort of the sort of any thing to be a more to have been a more the say and the sort of the say her at the sort of the say of the sort of the sort of her of the present of the sort of the present of the consertion of the sort of the sort of his father and such a sort of the say of the present of the say of the sort of his father, and she was not be all the present of the say of the sort of the say and the sort of the sort of his fairfax of the sort of the sort of her father and a such a sort of the present of the sort o\n",
            "Epoch 2. Loss: 1.4740708710781512\n",
            "the world with her for the say and she could not be a mind to be a mind to be all the say and she was not be all the sort of the present of the sort of the conserration of the sort of the sort of the sure of the sort of the sort of the sort of the sort of the sort of the sort of the sort of the sort of the sort of the present of the sort of the say of her father and so much and she was a sort of the sort of the say of the sort of the say and she was a sort of the say of the say of the say of the sort of his being to be a more to have been a more to have been the conserration of the sort of the sort of the present of the sort of the sort of the say of the sort of the say and the say of her father and she was a sort of his being to her fairfax of her father and so much and she was not the present of the say of his being to be a more to see of the say of her father and she was not be all the sort of the sort of the sort of the sort of the sort of her father and she was not be all the say and she was a more to any thing to be \n",
            "seemed to unite the sure of her father and so much and the consertion of the say and the say the say of the say her and the sort of the sort of the present of mr. weston was a sort of the sort of the sort of the consert of the sort of the sort of the sort of his being to have been a some to be a more to have been a very the consertion of the say of the sort of the sort of the say and she was not be all the say of the sort of the say of the sort of the say of her fairfax of the sort of harriet as she was a sort of the sort of the say of her father and she was so much and she had not be all the sort of the say of the sort of the say of his being to be a more to any thing to have been a did not be a more to have been a some to be a more to have been a sort of the sort of the sort of the present of the sort of the sort of the say and a some to be a more to have been a some to be a more to be a more to any thing of the sort of the sort of her father and the sort of the present of the sort of her fairfax of her father and so much\n",
            "Epoch 3. Loss: 1.4726779182201573\n",
            "the world with a sort of his being the such a sort of the sort of the sure of the could not be a mind to her at the sure of the conserration of her friends of the sort of the say she was to be a more to have been a some to be the sort of the say of the say and she was not be all the present of the sort of the say of the sort of the sort of the sort of the sort of her father and so much and so much as the say which she had not be all the seemed and she was not be a more to have been a did not be all the sort of the sort of his being to her at the sort of the present of the sort of his being to be a more to any the with a little and the consertion of her father and so being to be a more the sort of his being to be a more to have been so much and she was a sort of the sort of the say of the sort of the sort of the sort of the say of the party and the sort of the sort of the present of the sort of the sort of the sort of the sort of the present of the present of the consertion of the say of the say of the say of the sort of th\n",
            "seemed to unitely to be a more to have been a more to any the sort of the consertion of her father and so many and the sort of the sort of the present of the sort of the sort of the sort of the sort of the sort of his being to have been a great dear of the present of her father and the present of her father and so much and the present of the sort of the proper of the sort of the sort of the sort of the sort of the sort of his being to be a sort of his often and the sort of the say of the sort of the sort of the say of the say of the sort of the sort of the say and a more to have been a more to her at the sort of the sort of the sort of the sort of the sort of her father and she was to be a more the rest to her at the sure of the sort of the sort of the sort of the sort of the sort of the say which he had not be all the say of the sort of the same the sure of the say of the sort of the sort of the sort of her father and the sort of the sort of the restrable of the sort of the say and she was not be all the more the say and t\n",
            "Epoch 4. Loss: 1.4715713583697894\n",
            "the world with her the sort of the sort of the sort of his did not be a more to be a more to have been a some of the sure of the sort of her fairfax of the same the sort of the sort of a some of the say of the sort of her friends to have been a some of the sort of his being to be all the sort of the say of the say of the sort of the sort of the sort of the sort of the sort of the sort of the say her father and so being to have been a more to have been a more to have been a more to have been a very the such a sort of the sort of his being to be a more to have been a more to any thing to have been a very all the proper of the sort of her father and so much and she was not be all the say of her father and so much and she was a sort of the sort of her father and she was not be all the sort of the sort of the present of the consertion of his being to be a more to have been a some to her for the say of the say of the sort of the say and she was a sort of the sort of the sort of her father and the sort of the sort of his being to\n",
            "seemed to unitely and she was to be a more to have been a some to be a more to have been a some to be a more to have been a more the sort of a mind to be a more to have been a some to be a more to mane of the sort of the sort of the say of the say of the sort of the sort of the sort of her friends and the say her forthing and the say of the sure of the sort of the say and the same the sure of the present of the sort of the sort of the sort of the sort of his being to be a more to mane of the present of the say of the consertion of the sort of the present of the say and the consertion of the sort of the sort of the sure of the proper of the sort of the sort of the sort of the say of the sort of the sort of the proper of his being to be a more the sort of the sort of the say of the sort of his being to her at the sort of the sort of the same the sure of the partion and the sort of the sort of his being to be a more to any the party and the conserration of her friends of the sort of the say of the sort of the say of her father\n",
            "Epoch 5. Loss: 1.4706068522593012\n",
            "the world with her for the sort of a some of the sort of the sort of the say of her father and she was not be all the sort of her of the sort of the say of the sort of the sort of the sort of the satious of the sort of the sort of her father and in the sort of the sort of the sort of his being to be a more to have been a some to be a consert of the sort of her father and she was a sort of the say and the sure of the consertion of her of the sort of the present of the sort of a some of the say of the sort of the sort of the sort of his being to have been a very consertion of her father and she was not be a more to have been a some to her for the present of the say of the sort of the sort of the sort of the sort of her father and she was not be all the sort of the sort of her father and miss was the say of the sort of the sort of the sort of the sort of her faitfer and the say of the sort of the present of the sort of the sort of the sure of the sort of his being the proper of the sort of the sort of the sort of her father a\n",
            "seemed to unitely to be a more to be a sort of the sort of the sort of the sort of her father and she was not be all the sure of the sort of the sort of the sort of his being to be a more to have been a more to have been a manner the sure of his being to be a sort of the sort of the sort of his being to be such a sort of the sort of the sort of the sort of the sort of the same to be such a sort of the sort of the sort of the say of the sort of the sort of the sort of the say of the sort of his being to the present of the sort of the sort of her father and she was a sort of the sure of the sort of his being to be a more to have been a very seemed and such a some of the sort of the say of the sort of the consertion of her friends to be all the sort of the sort of his being to be a mind to have been a some of the say of the say and she was so been a some to be a more to any the sort of the sort of the sort of the sort of the sort of the sort of the sort of the sort of the sort of the sort of the sort of her father and so being\n",
            "Epoch 6. Loss: 1.4697294311486953\n",
            "the world with a sort of the sure of the sort of his being to her friends to her friends of the sort of the sort of his being the consertion of the sort of the sort of his being to have been a more to any the complessed to her friends to be a more to have been a sort of the sort of the sort of his such and she was not be all the sort of the same the sure of the sort of the sort of the sort of the sort of his ofter and so much as the say of the sort of her father and so much and the consert of his such and the sort of the say of her father and so much and such a sort of the sort of the present of the sort of the sort of a mist be as the sort of the sort of the sort of the sort of the say of a man the sure of the party and she was a sort of the sort of the present of the sort of her father and the sort of the say of the sort of the sort of the same the sure of the sort of the sort of the sort of the present of her father and she was a sort of the same the same the same the same the say and she was not the sort of a some of t\n",
            "seemed to unitely to mane of the say of the sort of the sort of the sort of the proper of the same to her at the same the say and she was not be all the proper of her father and she was not be all the sort of the sort of her father and the consertion of the say of the partion of his being to her father and so much and she was not be all the present of the sort of the say of her father and the sort of the sort of the sort of his being to be a more to have been a mind to her at the sort of the say which she had been a very such and she was not be all the say of his being to be a manner to be a more to have been a very such a sort of the say of the sort of his being to be a more the sort of the sort of the sort of her father and she had not such a sort of the consertion of her father and she was not be all the proper of her father and she was to be a more to any thing to her at the sort of the sort of the sort of the same to be a more to have been a very manner and the more the suppect of her father and she was not be all the \n",
            "Epoch 7. Loss: 1.468910446855121\n",
            "the world with her for her father and she was a sort of the sort of the sort of the say of the sort of the sort of the sort of the say of the conserration of the say of the same the sure of the sort of the sure of the same the suppect of the sort of the sort of her forthing of the consertion of the sort of her friends of the say of the say of the say of the sort of the sort of the sort of the sure of the sort of the sort of her friends the same to have been a more to have been a very such and the same the say and the say of the sort of the sort of the sort of the sort of the sort of the sort of the say and the sort of the say of the sort of her father and the consertion of her father and she was not the present of the supprion of her of the conserration of the consertion of the sort of her friends and she was a little of the sort of the partion of the same to her friends and she was not be all the same that he was a sort of the consertion of the say of the sort of her fairfax of the said the sure of the say of the sort of \n",
            "seemed to unitely the sort of the sort of the say of her father and so much and she was not be all the proper of his often and the conserration of her father and she was a sort of the sort of the sort of her of the sort of the present of the present of his being to her for the sort of the sort of the sort of the sort of the present of the sort of the sort of the same the say of her father and she was to be a more to have been a very some to have been a sort of her father and she had seen to be all the sure of the sort of the sort of the sure of the say of the sort of the sort of her father and the sort of the sort of the sort of the sort of the say of the sort of her father and she was not be all the present of her father and she could not be all the present of the sort of the sort of her father and so much and she was a sort of the sort of the sort of the sort of the sation of his being to be a sort of the sort of the same the suppect of her father and she was not be a more to have been a very all the say her of the conser\n",
            "Epoch 8. Loss: 1.4681330289673862\n",
            "the world with her friends to her at the sort of the sort of the sort of the sort of his being to be any the conserration of her father and she was not be and the consert of the sort of the sort of the sort of the sort of the sort of his being to her at the sort of her father and she was not be all the sort of the sort of the sating and the sort of her fairfax of her father and she was not be all the say of the sort of the sort of the sort of the sort of the sort of the sort of the sort of her of the consertion of the proper of the sort of the sort of the sort of the sort of her fairfax of her father and she was not be all the same the say and so much and she was not be all the sort of the sort of her father and she was not be all the sort of the sort of the sort of the sort of the sort of the say of her father and she was not be all the proper of the sort of the say of her father and the sort of the sort of the sort of his being to be a sort of the sort of the sure of the sort of the sort of the say and the sort of her fa\n",
            "seemed to unite the such a sort of the sort of the sort of the sort of the sure of the same the more the same the say of the say of his being to her at the sort of her father and she was a sort of the say of the sort of the sort of the sort of the sort of the same the sure of the sort of the sort of her father and she was a sort of the sort of the say of the sort of his being to her father and she was not be all the sort of the say and she was a sort of the sort of the sort of his often and the sort of the say of the same the suppect of his being to be a manner the say of the sort of the sort of the proper of her friends the say of the consertion of the sort of his being to her at the sort of the proper of the sort of the sort of her of the present of the sort of the sort of the say of the sort of the sure of the sort of the sort of the sort of the sort of the sort of the such and she was not be all the sort of the say of the sort of his being to be all the say her father and she was not be all the sort of the conserration \n",
            "Epoch 9. Loss: 1.4673865680474654\n",
            "the world with her for the sort of the sort of the say of the sort of the sort of the same the sure of the sort of her fairfax of her father and the say and the sort of the present of the present of the sort of his being to be a more the sort of the sort of the proper of the consertion of her of the conserration of her friends of the sort of the sort of her father was a sort of the say of the sure of the said the suppect of her at the sort of the sort of the say of the sort of the sort of the sort of the sort of her of the sort of the sort of the sort of her father and she was a sort of the say of the say of the say of the sort of the say of her father and the present of the say of the sort of the same to her at the sort of the say and so much and she had been a sort of the sort of his being to be a more to have been a some to have been a more to have been a more to have been a more to be a more to have been a sort of the sort of the sort of her friends to have been a more to have been a more to have been a some to be a mo\n",
            "seemed to unite the such a sort of the sort of the sort of his being to the consertion of her at the sort of the same the sure of the sort of the same the sure of the say and the same the say of the sort of the sort of the sort of her father and so much as the say of the say of the same the sure of the sort of his being to have been a some to be a more to have been a some to be a more the sure of the sort of her father and the consertion of his being to have been a sort of the conserration of the say of her father and she was not be all the sort of his father and the consert of the sort of his being to her father and the sort of the sort of the sort of a mist be as the present of her friends and her father and she was a sort of the say of the sort of her of the same the say of the say of the say she was not be all the sort of the sort of her at the sort of the sort of his being to be a mind to be all the sort of the sort of the say of the sort of the sort of his being to her father and the sort of the say of her father and \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### GRU"
      ],
      "metadata": {
        "id": "MELFX49Z5EYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_inputs = vocab_size\n",
        "num_hidden = 256\n",
        "num_outputs = vocab_size\n",
        "\n",
        "########################\n",
        "#  Weights connecting the inputs to the hidden layer\n",
        "########################\n",
        "Wxz = nd.random_normal(shape=(num_inputs,num_hidden), ctx=ctx) * .05\n",
        "Wxr = nd.random_normal(shape=(num_inputs,num_hidden), ctx=ctx) * .05\n",
        "Wxh = nd.random_normal(shape=(num_inputs,num_hidden), ctx=ctx) * .05\n",
        "\n",
        "########################\n",
        "#  Recurrent weights connecting the hidden layer across time steps\n",
        "########################\n",
        "Whz = nd.random_normal(shape=(num_hidden,num_hidden), ctx=ctx)* .05\n",
        "Whr = nd.random_normal(shape=(num_hidden,num_hidden), ctx=ctx)* .05\n",
        "Whh = nd.random_normal(shape=(num_hidden,num_hidden), ctx=ctx)* .05\n",
        "\n",
        "########################\n",
        "#  Bias vector for hidden layer\n",
        "########################\n",
        "bz = nd.random_normal(shape=num_hidden, ctx=ctx) * .05\n",
        "br = nd.random_normal(shape=num_hidden, ctx=ctx) * .05\n",
        "bh = nd.random_normal(shape=num_hidden, ctx=ctx) * .05\n",
        "\n",
        "########################\n",
        "# Weights to the output nodes\n",
        "########################\n",
        "Why = nd.random_normal(shape=(num_hidden,num_outputs), ctx=ctx) * .05\n",
        "by = nd.random_normal(shape=num_outputs, ctx=ctx) * .05\n",
        "\n",
        "params = [Wxz, Wxr, Wxh, Whz, Whr, Whh, bz, br, bh, Why, by]\n",
        "\n",
        "for param in params:\n",
        "    param.attach_grad()\n",
        "\n",
        "def gru_rnn(inputs, h, temperature=1.0):\n",
        "    outputs = []\n",
        "    for X in inputs:\n",
        "        z = nd.sigmoid(nd.dot(X, Wxz) + nd.dot(h, Whz) + bz)\n",
        "        r = nd.sigmoid(nd.dot(X, Wxr) + nd.dot(h, Whr) + br)\n",
        "        g = nd.tanh(nd.dot(X, Wxh) + nd.dot(r * h, Whh) + bh)\n",
        "        h = z * h + (1 - z) * g\n",
        "\n",
        "        yhat_linear = nd.dot(h, Why) + by\n",
        "        yhat = softmax(yhat_linear, temperature=temperature)\n",
        "        outputs.append(yhat)\n",
        "    return (outputs, h)"
      ],
      "metadata": {
        "id": "Z7Rvxo9D5Cs3"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "moving_loss = 0.1\n",
        "learning_rate = 0.5\n",
        "\n",
        "# state = nd.zeros(shape=(batch_size, num_hidden), ctx=ctx)\n",
        "for e in range(epochs):\n",
        "    ############################\n",
        "    # Attenuate the learning rate by a factor of 2 every 100 epochs.\n",
        "    ############################\n",
        "    if ((e+1) % 100 == 0):\n",
        "        learning_rate = learning_rate / 2.0\n",
        "    h = nd.zeros(shape=(batch_size, num_hidden), ctx=ctx)\n",
        "    for i in range(num_batches):\n",
        "        data_one_hot = train_data[i]\n",
        "        label_one_hot = train_label[i]\n",
        "        with autograd.record():\n",
        "            outputs, h = gru_rnn(data_one_hot, h)\n",
        "            loss = average_ce_loss(outputs, label_one_hot)\n",
        "            loss.backward()\n",
        "        SGD(params, learning_rate)\n",
        "\n",
        "        ##########################\n",
        "        #  Keep a moving average of the losses\n",
        "        ##########################\n",
        "        if (i == 0) and (e == 0):\n",
        "            moving_loss = nd.mean(loss).asscalar()\n",
        "        else:\n",
        "            moving_loss = .99 * moving_loss + .01 * nd.mean(loss).asscalar()\n",
        "\n",
        "    print(\"Epoch %s. Loss: %s\" % (e, moving_loss))\n",
        "    print(sample(\"the world with\", 10, temperature=.1))\n",
        "    print(sample(\"seemed to unite\", 10, temperature=.1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvg6shre5JPY",
        "outputId": "ffc17807-ed57-46ab-be80-4871ce49803b"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0. Loss: 2.8041779993688407\n",
            "the world with 6 mmh b j\n",
            "seemed to unite mshli t h\n",
            "Epoch 1. Loss: 2.4447447842446284\n",
            "the world withw whhhllgg\n",
            "seemed to unite -hhhhildg\n",
            "Epoch 2. Loss: 2.2963930643724884\n",
            "the world with u hhhllgg\n",
            "seemed to unite -hhhildd \n",
            "Epoch 3. Loss: 2.188511645769327\n",
            "the world withu mdhlii u\n",
            "seemed to unite--hnhsssst\n",
            "Epoch 4. Loss: 2.0936804726011693\n",
            "the world with u hhhlld-\n",
            "seemed to unite \" hghlhdy\n",
            "Epoch 5. Loss: 2.012501041469105\n",
            "the world with u hhhlild\n",
            "seemed to unite--hchssst-\n",
            "Epoch 6. Loss: 1.941249763237411\n",
            "the world with pghr tgf \n",
            "seemed to unite -hhhildd \n",
            "Epoch 7. Loss: 1.879167965545937\n",
            "the world with pgwrl t u\n",
            "seemed to unitet-hnnssttn\n",
            "Epoch 8. Loss: 1.8227160805102254\n",
            "the world with u hhhildd\n",
            "seemed to unitet-hnnssssa\n",
            "Epoch 9. Loss: 1.770420153523318\n",
            "the world with pgwrl t p\n",
            "seemed to unitet--knnttdt\n"
          ]
        }
      ]
    }
  ]
}